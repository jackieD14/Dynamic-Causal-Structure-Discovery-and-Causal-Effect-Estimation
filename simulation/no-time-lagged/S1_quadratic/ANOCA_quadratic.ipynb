{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14283b70",
   "metadata": {
    "id": "14283b70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "D:\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89455d",
   "metadata": {
    "id": "1d89455d"
   },
   "source": [
    "## create basis & function demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da3d05e",
   "metadata": {
    "id": "7da3d05e"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461fc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from torch.optim.adam import Adam\n",
    "from utils import *\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1a244e",
   "metadata": {
    "id": "6d1a244e"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "def cos(x):\n",
    "    return(-10+(5-x)**2)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f498d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "b1f498d7",
    "outputId": "337ffdf2-8eee-4e75-97b3-51ae278a7cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: [ 0.   0.   0.   2.5  5.   7.5 10.  10.  10. ]\n",
      "c: [ 0.75   0.125 -0.5   -0.5    0.125  0.75   0.     0.     0.   ]\n",
      "k: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree =2\n",
    "x = np.array(range(0, 11))\n",
    "y = np.array([cos(i) for i in x])\n",
    "z= [cos(i/10) for i in np.array(range(0, 110))]\n",
    "m = 3\n",
    "step = (x[-1] - x[0]) / (m + 1)\n",
    "knots = np.linspace(step, m * step, m)\n",
    "\n",
    "t, c, k = interpolate.splrep(x, y, k=degree, s=0, t=knots, per=0)\n",
    "\n",
    "print('''\\\n",
    "t: {}\n",
    "c: {}\n",
    "k: {}\n",
    "'''.format(t, c, k))\n",
    "N = 100\n",
    "xmin, xmax = x.min(), x.max()\n",
    "xx = np.linspace(xmin, xmax, N)\n",
    "spline = interpolate.BSpline(t, c, k, extrapolate=False)\n",
    "\n",
    "plt.plot(x, y, label='data points')\n",
    "plt.plot(xx, spline(xx), label='BSpline')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcf3e1",
   "metadata": {
    "id": "e1fcf3e1"
   },
   "source": [
    "## create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df94f92c",
   "metadata": {
    "id": "df94f92c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from torch.optim.adam import Adam\n",
    "from utils import *\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "V6gVN_VTxnG4",
   "metadata": {
    "id": "V6gVN_VTxnG4"
   },
   "outputs": [],
   "source": [
    "def data_create(seed,time_stamp):\n",
    "  np.random.seed(seed)\n",
    "  x = np.array(range(0, time_stamp))\n",
    "  y = np.array([cos(i) for i in x]) #generate coefficient\n",
    "\n",
    "  base_DAG=np.zeros((5, 5))\n",
    "  base_DAG[0,4]=y[0]##edited to be coeffcient with error\n",
    "  base_graph=nx.from_numpy_matrix(base_DAG,create_using=nx.DiGraph)\n",
    "  X_all = simulate_lsem(base_graph,30, 'Binary', 1,noise_scale=0.1)\n",
    "  for i in range(1,time_stamp):\n",
    "      base_DAG[0,4]=y[i]\n",
    "      base_graph=nx.from_numpy_matrix(base_DAG,create_using=nx.DiGraph)\n",
    "      X = simulate_lsem(base_graph,30, 'Binary', 1,noise_scale=0.1)\n",
    "      X_all=np.append(X_all,X,axis=0)\n",
    "  return X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30 # The number of samples of data.\n",
    "d = 5 # The number of variables in data.\n",
    "time_stamp=10 #no. of timestamp\n",
    "np.random.seed(1234567) #Random seed\n",
    "n_times=30\n",
    "seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "seed=seed_list[0]\n",
    "X_all=data_create(seed,time_stamp) #create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675a8b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([913812, 374400, 343669, 289095, 235846, 432099, 448097, 384097,\n",
       "       134030, 454991, 401627,  97188, 615884, 585262, 902641, 897795,\n",
       "       678150, 361884, 928159, 446640,  26580, 865407, 789523, 704840,\n",
       "       359269,  38248, 809111, 137636, 698530, 230177])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b20ee4",
   "metadata": {
    "id": "94b20ee4"
   },
   "source": [
    "## piecewise ANOCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1766d8b",
   "metadata": {
    "id": "f1766d8b"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "from utils import *\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "from numpy.random import randn\n",
    "from random import seed as rseed\n",
    "from numpy.random import seed as npseed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8bde47",
   "metadata": {
    "id": "cc8bde47"
   },
   "outputs": [],
   "source": [
    "def train(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr):\n",
    "        \n",
    "        nll_train = []\n",
    "        kl_train = []\n",
    "        mse_train = []\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update optimizer\n",
    "        optimizer, lr = update_optimizer(optimizer, old_lr, c_B, d_B)\n",
    "\n",
    "        for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "            data, relations = Variable(data).double(), Variable(relations).double()\n",
    "            relations = relations.unsqueeze(2) # Reshape data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_x, logits, origin_B, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data, rel_rec, rel_send) \n",
    "            edges = logits # Logits is of size: [num_sims, z_dims]\n",
    "\n",
    "            dec_x, output, adj_A_tilt_decoder = decoder(data, edges, d * x_dims, rel_rec, rel_send, origin_B, adj_A_tilt_encoder, Wa)\n",
    "\n",
    "            if torch.sum(output != output):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            target = data\n",
    "            preds = output\n",
    "            variance = 0.\n",
    "            \n",
    "            # Compute constraint functions h1(B) and h2(B)\n",
    "            h1_B = fun_h1_B(origin_B)\n",
    "            h2_B = fun_h2_B(origin_B)\n",
    "\n",
    "            # Reconstruction accuracy loss:\n",
    "            loss_nll = nll_gaussian(preds, target, variance)\n",
    "            # KL loss:\n",
    "            loss_kl = kl_gaussian(logits)\n",
    "            # ELBO loss:\n",
    "            loss = loss_kl + loss_nll\n",
    "            # Loss function:\n",
    "            loss += lambda1 * h1_B + 0.5 * c_B * h1_B * h1_B + lambda2 * h2_B + 0.5 * d_B * h2_B * h2_B + 100. * torch.trace(origin_B * origin_B)\n",
    "\n",
    "            loss.backward()\n",
    "            loss = optimizer.step()\n",
    "\n",
    "            myA.data = stau(myA.data, tau_B * lr)\n",
    "\n",
    "            if torch.sum(origin_B != origin_B):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            mse_train.append(F.mse_loss(preds, target).item())\n",
    "            nll_train.append(loss_nll.item())\n",
    "            kl_train.append(loss_kl.item())\n",
    "\n",
    "        return np.mean(np.mean(kl_train) + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), origin_B, optimizer, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JHrk3hsw6IB4",
   "metadata": {
    "id": "JHrk3hsw6IB4"
   },
   "source": [
    "import times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "AWxKN2Pm6J6z",
   "metadata": {
    "id": "AWxKN2Pm6J6z"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c87d9f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c87d9f5",
    "outputId": "85d2eeeb-b081-4090-e0f6-ceb791280937",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.0008666900919097248\n",
      "Best NLL Loss : 2.9408539417944714e-06\n",
      "Best MSE Loss : 1.1763415767177888e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0010375961565224174\n",
      "Best NLL Loss : 2.3431152047429383e-06\n",
      "Best MSE Loss : 9.372460818971755e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0007172442242579296\n",
      "Best NLL Loss : 1.1937854663570226e-06\n",
      "Best MSE Loss : 4.775141865428092e-07\n",
      "2\n",
      "Best ELBO Loss : 0.000987210965158249\n",
      "Best NLL Loss : 2.6842275136994995e-06\n",
      "Best MSE Loss : 1.0736910054797993e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0008972870561138909\n",
      "Best NLL Loss : 2.475291676778531e-06\n",
      "Best MSE Loss : 9.901166707114123e-07\n",
      "4\n",
      "Best ELBO Loss : 0.00109961746340621\n",
      "Best NLL Loss : 3.8875259236775516e-06\n",
      "Best MSE Loss : 1.5550103694710208e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0009157477585176695\n",
      "Best NLL Loss : 2.0187972425735836e-06\n",
      "Best MSE Loss : 8.075188970294333e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0007855837019902626\n",
      "Best NLL Loss : 3.2404914454462825e-06\n",
      "Best MSE Loss : 1.2961965781785129e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0009014797832237255\n",
      "Best NLL Loss : 1.2424764221557886e-06\n",
      "Best MSE Loss : 4.969905688623155e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0008034831568401237\n",
      "Best NLL Loss : 9.015111826773561e-07\n",
      "Best MSE Loss : 3.6060447307094257e-07\n",
      "9\n",
      "0\n",
      "374400\n",
      "Best ELBO Loss : 0.002362455970380906\n",
      "Best NLL Loss : 3.1583072387878555e-06\n",
      "Best MSE Loss : 1.2633228955151423e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0024906849279705103\n",
      "Best NLL Loss : 6.298343739653e-06\n",
      "Best MSE Loss : 2.5193374958612e-06\n",
      "1\n",
      "Best ELBO Loss : 0.002689399271509194\n",
      "Best NLL Loss : 7.566579455589489e-06\n",
      "Best MSE Loss : 3.026631782235796e-06\n",
      "2\n",
      "Best ELBO Loss : 0.002249806823887362\n",
      "Best NLL Loss : 5.916600995063462e-06\n",
      "Best MSE Loss : 2.366640398025385e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0017864597923094691\n",
      "Best NLL Loss : 4.149765671517084e-06\n",
      "Best MSE Loss : 1.6599062686068333e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0018537750677316317\n",
      "Best NLL Loss : 3.4844359261702305e-06\n",
      "Best MSE Loss : 1.3937743704680917e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001504166606901087\n",
      "Best NLL Loss : 3.3177766815900828e-06\n",
      "Best MSE Loss : 1.327110672636033e-06\n",
      "6\n",
      "Best ELBO Loss : 0.00197973806552101\n",
      "Best NLL Loss : 4.600477223793293e-06\n",
      "Best MSE Loss : 1.8401908895173172e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0018674291896442921\n",
      "Best NLL Loss : 4.565388806831261e-06\n",
      "Best MSE Loss : 1.826155522732504e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0021937187275075623\n",
      "Best NLL Loss : 5.681359673986537e-06\n",
      "Best MSE Loss : 2.2725438695946146e-06\n",
      "9\n",
      "1\n",
      "343669\n",
      "Best ELBO Loss : 0.0014807424516091769\n",
      "Best NLL Loss : 1.8841781176943553e-06\n",
      "Best MSE Loss : 7.536712470777421e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0010563363190669803\n",
      "Best NLL Loss : 5.332433159694991e-07\n",
      "Best MSE Loss : 2.1329732638779963e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0006357067938683151\n",
      "Best NLL Loss : 7.333382659635145e-07\n",
      "Best MSE Loss : 2.9333530638540584e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0010761086346541878\n",
      "Best NLL Loss : 2.107476227941408e-06\n",
      "Best MSE Loss : 8.429904911765633e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0010891835698984577\n",
      "Best NLL Loss : 1.9160639501134317e-06\n",
      "Best MSE Loss : 7.664255800453725e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0010554861145172583\n",
      "Best NLL Loss : 1.6028113743621058e-06\n",
      "Best MSE Loss : 6.411245497448423e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0013186892789330443\n",
      "Best NLL Loss : 1.993388975703173e-06\n",
      "Best MSE Loss : 7.973555902812694e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0009281612092592994\n",
      "Best NLL Loss : 1.4273101722183995e-06\n",
      "Best MSE Loss : 5.709240688873597e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0009037251590673579\n",
      "Best NLL Loss : 1.6881038115590593e-06\n",
      "Best MSE Loss : 6.752415246236239e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0008982444542921599\n",
      "Best NLL Loss : 1.6437074651363093e-06\n",
      "Best MSE Loss : 6.574829860545238e-07\n",
      "9\n",
      "2\n",
      "289095\n",
      "Best ELBO Loss : 0.0011790444920545858\n",
      "Best NLL Loss : 1.02688946008508e-05\n",
      "Best MSE Loss : 4.107557840340321e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0008306313547761503\n",
      "Best NLL Loss : 3.328723723045566e-06\n",
      "Best MSE Loss : 1.331489489218227e-06\n",
      "1\n",
      "Best ELBO Loss : 0.001663084552500185\n",
      "Best NLL Loss : 3.951007870925187e-06\n",
      "Best MSE Loss : 1.5804031483700748e-06\n",
      "2\n",
      "Best ELBO Loss : 0.0012280559240390417\n",
      "Best NLL Loss : 2.7731659751696017e-06\n",
      "Best MSE Loss : 1.1092663900678407e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0015192502258346387\n",
      "Best NLL Loss : 3.2221383255981335e-05\n",
      "Best MSE Loss : 1.2888553302392533e-05\n",
      "4\n",
      "Best ELBO Loss : 0.0012517742350774887\n",
      "Best NLL Loss : 5.86900344373832e-06\n",
      "Best MSE Loss : 2.347601377495327e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001303641264380491\n",
      "Best NLL Loss : 5.769990546627691e-05\n",
      "Best MSE Loss : 2.307996218651076e-05\n",
      "6\n",
      "Best ELBO Loss : 0.0015971631645522607\n",
      "Best NLL Loss : 3.865098432571036e-06\n",
      "Best MSE Loss : 1.546039373028414e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0008568631894710752\n",
      "Best NLL Loss : 1.643429801704496e-06\n",
      "Best MSE Loss : 6.573719206817985e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0013065891697704743\n",
      "Best NLL Loss : 4.9629799065034574e-05\n",
      "Best MSE Loss : 1.9851919626013822e-05\n",
      "9\n",
      "3\n",
      "235846\n",
      "Best ELBO Loss : 0.00115501140509781\n",
      "Best NLL Loss : 3.2529691265391683e-06\n",
      "Best MSE Loss : 1.3011876506156676e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0013421693029593113\n",
      "Best NLL Loss : 4.165523738280137e-06\n",
      "Best MSE Loss : 1.6662094953120544e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0008618683583493389\n",
      "Best NLL Loss : 1.563818421744307e-06\n",
      "Best MSE Loss : 6.255273686977227e-07\n",
      "2\n",
      "Best ELBO Loss : 0.001035804851027225\n",
      "Best NLL Loss : 1.7993438012917406e-06\n",
      "Best MSE Loss : 7.197375205166963e-07\n",
      "3\n",
      "Best ELBO Loss : 0.001421693667398217\n",
      "Best NLL Loss : 3.2436945871477292e-06\n",
      "Best MSE Loss : 1.2974778348590917e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0011725739754868294\n",
      "Best NLL Loss : 2.1665808013809538e-06\n",
      "Best MSE Loss : 8.666323205523813e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0010526164570969388\n",
      "Best NLL Loss : 1.9519954145586197e-06\n",
      "Best MSE Loss : 7.807981658234479e-07\n",
      "6\n",
      "Best ELBO Loss : 0.001197411733472443\n",
      "Best NLL Loss : 2.1496620753369253e-06\n",
      "Best MSE Loss : 8.5986483013477e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0011274224386886372\n",
      "Best NLL Loss : 2.0258687402401187e-06\n",
      "Best MSE Loss : 8.103474960960475e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0012327575528928367\n",
      "Best NLL Loss : 2.5163152428535085e-06\n",
      "Best MSE Loss : 1.0065260971414034e-06\n",
      "9\n",
      "4\n",
      "432099\n",
      "Best ELBO Loss : 0.0013472157960229562\n",
      "Best NLL Loss : 2.807818128261844e-06\n",
      "Best MSE Loss : 1.1231272513047374e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0011648927172671103\n",
      "Best NLL Loss : 2.3337007762290894e-06\n",
      "Best MSE Loss : 9.334803104916357e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0012208666415004722\n",
      "Best NLL Loss : 2.4307159438633563e-06\n",
      "Best MSE Loss : 9.722863775453425e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0012777719027533857\n",
      "Best NLL Loss : 3.0505141473446357e-06\n",
      "Best MSE Loss : 1.2202056589378543e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0013648331837044655\n",
      "Best NLL Loss : 2.549033261297563e-06\n",
      "Best MSE Loss : 1.0196133045190255e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0011719078937168816\n",
      "Best NLL Loss : 2.8271564965441204e-06\n",
      "Best MSE Loss : 1.1308625986176485e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0013009935340831513\n",
      "Best NLL Loss : 2.7722316740244194e-06\n",
      "Best MSE Loss : 1.1088926696097679e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0011996215211456005\n",
      "Best NLL Loss : 3.3787455848702147e-06\n",
      "Best MSE Loss : 1.3514982339480858e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0016985542733108014\n",
      "Best NLL Loss : 3.6148801236443578e-06\n",
      "Best MSE Loss : 1.4459520494577434e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0016592306813288397\n",
      "Best NLL Loss : 3.211248009659586e-06\n",
      "Best MSE Loss : 1.2844992038638344e-06\n",
      "9\n",
      "5\n",
      "448097\n",
      "Best ELBO Loss : 0.0007796919602764977\n",
      "Best NLL Loss : 5.281673725107786e-07\n",
      "Best MSE Loss : 2.1126694900431146e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0008860714422726069\n",
      "Best NLL Loss : 1.4274777635275673e-06\n",
      "Best MSE Loss : 5.709911054110268e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0010560182632620295\n",
      "Best NLL Loss : 1.4872626122396263e-06\n",
      "Best MSE Loss : 5.949050448958506e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0007447166225728016\n",
      "Best NLL Loss : 1.1375270044871158e-06\n",
      "Best MSE Loss : 4.5501080179484623e-07\n",
      "3\n",
      "Best ELBO Loss : 0.000905719745033862\n",
      "Best NLL Loss : 1.703065425171655e-06\n",
      "Best MSE Loss : 6.812261700686621e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0009144297817619744\n",
      "Best NLL Loss : 1.326107134731138e-06\n",
      "Best MSE Loss : 5.304428538924554e-07\n",
      "5\n",
      "Best ELBO Loss : 0.000940992182749718\n",
      "Best NLL Loss : 8.317361193649842e-07\n",
      "Best MSE Loss : 3.326944477459938e-07\n",
      "6\n",
      "Best ELBO Loss : 0.001048022753440796\n",
      "Best NLL Loss : 1.599653666673838e-06\n",
      "Best MSE Loss : 6.398614666695352e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0010761282179291777\n",
      "Best NLL Loss : 1.7124291017782421e-06\n",
      "Best MSE Loss : 6.849716407112967e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0011654247511093143\n",
      "Best NLL Loss : 1.548224451198817e-06\n",
      "Best MSE Loss : 6.192897804795269e-07\n",
      "9\n",
      "6\n",
      "384097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.0016709769686480192\n",
      "Best NLL Loss : 1.8797258345049285e-05\n",
      "Best MSE Loss : 7.518903338019715e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0009533024912797004\n",
      "Best NLL Loss : 3.00425819130265e-06\n",
      "Best MSE Loss : 1.2017032765210603e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0006595144361247456\n",
      "Best NLL Loss : 8.480078652582857e-07\n",
      "Best MSE Loss : 3.392031461033143e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0011307716149302099\n",
      "Best NLL Loss : 2.2202262192563785e-06\n",
      "Best MSE Loss : 8.880904877025514e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0015538698461124367\n",
      "Best NLL Loss : 1.7241093916315134e-05\n",
      "Best MSE Loss : 6.896437566526052e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0015102725416025556\n",
      "Best NLL Loss : 1.2210067102994107e-05\n",
      "Best MSE Loss : 4.884026841197643e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0011865276845419764\n",
      "Best NLL Loss : 3.1616274387759505e-06\n",
      "Best MSE Loss : 1.2646509755103802e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0009269915968505493\n",
      "Best NLL Loss : 5.240059959237185e-06\n",
      "Best MSE Loss : 2.096023983694874e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0005974345321823323\n",
      "Best NLL Loss : 1.420552133402068e-06\n",
      "Best MSE Loss : 5.682208533608271e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0012343648466731712\n",
      "Best NLL Loss : 2.8395324188645866e-06\n",
      "Best MSE Loss : 1.1358129675458347e-06\n",
      "9\n",
      "7\n",
      "134030\n",
      "Best ELBO Loss : 0.0010420127604489193\n",
      "Best NLL Loss : 6.958178543070697e-07\n",
      "Best MSE Loss : 2.7832714172282783e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0011108151963099102\n",
      "Best NLL Loss : 2.185261575624584e-06\n",
      "Best MSE Loss : 8.741046302498336e-07\n",
      "1\n",
      "Best ELBO Loss : 0.001381912960010198\n",
      "Best NLL Loss : 2.5140833686560613e-06\n",
      "Best MSE Loss : 1.0056333474624246e-06\n",
      "2\n",
      "Best ELBO Loss : 0.0011931796416639483\n",
      "Best NLL Loss : 2.093777176887954e-06\n",
      "Best MSE Loss : 8.375108707551814e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0008370202035867378\n",
      "Best NLL Loss : 2.0270573168851916e-06\n",
      "Best MSE Loss : 8.108229267540768e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0012409232706409293\n",
      "Best NLL Loss : 2.6789689280139257e-06\n",
      "Best MSE Loss : 1.0715875712055704e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0012314450805727402\n",
      "Best NLL Loss : 2.595994679425358e-06\n",
      "Best MSE Loss : 1.0383978717701434e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0011010028184755985\n",
      "Best NLL Loss : 2.326927523688783e-06\n",
      "Best MSE Loss : 9.307710094755134e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0013824346321794853\n",
      "Best NLL Loss : 2.3055520785822623e-06\n",
      "Best MSE Loss : 9.222208314329047e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0009436819464755358\n",
      "Best NLL Loss : 8.185915903278938e-07\n",
      "Best MSE Loss : 3.274366361311575e-07\n",
      "9\n",
      "8\n",
      "454991\n",
      "Best ELBO Loss : 0.0015184526114408422\n",
      "Best NLL Loss : 2.3027595474811017e-06\n",
      "Best MSE Loss : 9.211038189924409e-07\n",
      "0\n",
      "Best ELBO Loss : 0.001169355317892665\n",
      "Best NLL Loss : 1.6847945386676738e-06\n",
      "Best MSE Loss : 6.739178154670695e-07\n",
      "1\n",
      "Best ELBO Loss : 0.001194763208985202\n",
      "Best NLL Loss : 1.6671458292131573e-06\n",
      "Best MSE Loss : 6.668583316852628e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0010066358633688088\n",
      "Best NLL Loss : 1.5319855090391727e-06\n",
      "Best MSE Loss : 6.127942036156692e-07\n",
      "3\n",
      "Best ELBO Loss : 0.00131270606736679\n",
      "Best NLL Loss : 1.5393502038018926e-06\n",
      "Best MSE Loss : 6.157400815207569e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0013385562112365522\n",
      "Best NLL Loss : 1.5483546604880897e-06\n",
      "Best MSE Loss : 6.193418641952359e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0013125440747137939\n",
      "Best NLL Loss : 1.8646002339554451e-06\n",
      "Best MSE Loss : 7.458400935821781e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0012141966005076023\n",
      "Best NLL Loss : 1.705188404387666e-06\n",
      "Best MSE Loss : 6.820753617550663e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0011066471784846978\n",
      "Best NLL Loss : 1.6991530192730185e-06\n",
      "Best MSE Loss : 6.796612077092073e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0010983781296634704\n",
      "Best NLL Loss : 1.5654866880783068e-06\n",
      "Best MSE Loss : 6.261946752313228e-07\n",
      "9\n",
      "9\n",
      "401627\n",
      "Best ELBO Loss : 0.0009444017281824104\n",
      "Best NLL Loss : 1.8399532524240709e-06\n",
      "Best MSE Loss : 7.359813009696284e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0008662650124603018\n",
      "Best NLL Loss : 1.219402260912826e-06\n",
      "Best MSE Loss : 4.877609043651304e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0008744944943766407\n",
      "Best NLL Loss : 1.0196106676909531e-06\n",
      "Best MSE Loss : 4.0784426707638135e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0011346633512798892\n",
      "Best NLL Loss : 1.3312770181112145e-06\n",
      "Best MSE Loss : 5.325108072444859e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0007658570460310966\n",
      "Best NLL Loss : 9.836563205397588e-07\n",
      "Best MSE Loss : 3.934625282159036e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0007796083846472512\n",
      "Best NLL Loss : 1.0069631574335745e-06\n",
      "Best MSE Loss : 4.0278526297342984e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0008622859237766423\n",
      "Best NLL Loss : 1.784730285580292e-06\n",
      "Best MSE Loss : 7.138921142321168e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0008625308915435552\n",
      "Best NLL Loss : 1.18228409053258e-06\n",
      "Best MSE Loss : 4.7291363621303204e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0011840583600543021\n",
      "Best NLL Loss : 2.2797350056012416e-06\n",
      "Best MSE Loss : 9.118940022404973e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0009056963412027109\n",
      "Best NLL Loss : 1.0088738453514864e-06\n",
      "Best MSE Loss : 4.0354953814059445e-07\n",
      "9\n",
      "10\n",
      "97188\n",
      "Best ELBO Loss : 0.0013814144289713524\n",
      "Best NLL Loss : 2.6433851326845172e-06\n",
      "Best MSE Loss : 1.0573540530738068e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0012352105170784608\n",
      "Best NLL Loss : 1.6947461991638884e-06\n",
      "Best MSE Loss : 6.778984796655553e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0013008629357683943\n",
      "Best NLL Loss : 2.2158808491178665e-06\n",
      "Best MSE Loss : 8.863523396471465e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0015126675189089459\n",
      "Best NLL Loss : 2.932119640822448e-06\n",
      "Best MSE Loss : 1.1728478563289793e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0017161640285700897\n",
      "Best NLL Loss : 2.8968020971725074e-06\n",
      "Best MSE Loss : 1.158720838869003e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0014982638293111325\n",
      "Best NLL Loss : 3.1248036968843426e-06\n",
      "Best MSE Loss : 1.2499214787537372e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0011682800309406717\n",
      "Best NLL Loss : 1.9359004073671903e-06\n",
      "Best MSE Loss : 7.743601629468763e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0010347223505381576\n",
      "Best NLL Loss : 1.99601549573813e-06\n",
      "Best MSE Loss : 7.98406198295252e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0011855215836557304\n",
      "Best NLL Loss : 2.005226019047291e-06\n",
      "Best MSE Loss : 8.020904076189164e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0012424383597894337\n",
      "Best NLL Loss : 2.3078403299037433e-06\n",
      "Best MSE Loss : 9.231361319614973e-07\n",
      "9\n",
      "11\n",
      "615884\n",
      "Best ELBO Loss : 0.0010789209570346702\n",
      "Best NLL Loss : 1.3601636951985784e-06\n",
      "Best MSE Loss : 5.440654780794313e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0013623540809571894\n",
      "Best NLL Loss : 2.147825810039594e-06\n",
      "Best MSE Loss : 8.591303240158376e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0009367476742619394\n",
      "Best NLL Loss : 1.7042687882366227e-06\n",
      "Best MSE Loss : 6.817075152946492e-07\n",
      "2\n",
      "Best ELBO Loss : 0.001413059994580733\n",
      "Best NLL Loss : 2.055213924945811e-06\n",
      "Best MSE Loss : 8.220855699783244e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0010857702703127704\n",
      "Best NLL Loss : 1.4450885963700853e-06\n",
      "Best MSE Loss : 5.780354385480343e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0008288989319668251\n",
      "Best NLL Loss : 1.0981630235745195e-06\n",
      "Best MSE Loss : 4.3926520942980773e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0011856424002838466\n",
      "Best NLL Loss : 1.7522734968969477e-06\n",
      "Best MSE Loss : 7.009093987587791e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0010984954966541218\n",
      "Best NLL Loss : 1.5068267004673076e-06\n",
      "Best MSE Loss : 6.02730680186923e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0010338315103678793\n",
      "Best NLL Loss : 1.861203451807373e-06\n",
      "Best MSE Loss : 7.444813807229491e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0011284319813368456\n",
      "Best NLL Loss : 1.8193614944741576e-06\n",
      "Best MSE Loss : 7.277445977896631e-07\n",
      "9\n",
      "12\n",
      "585262\n",
      "Best ELBO Loss : 0.0011885572735547683\n",
      "Best NLL Loss : 3.382377570935337e-06\n",
      "Best MSE Loss : 1.3529510283741353e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0010876548345445027\n",
      "Best NLL Loss : 2.3734756415051983e-06\n",
      "Best MSE Loss : 9.493902566020794e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0008914607062924093\n",
      "Best NLL Loss : 1.4969009160276278e-06\n",
      "Best MSE Loss : 5.987603664110513e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0015107911021123729\n",
      "Best NLL Loss : 2.931607810766458e-06\n",
      "Best MSE Loss : 1.1726431243065835e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0010668725818521335\n",
      "Best NLL Loss : 2.2771958803344635e-06\n",
      "Best MSE Loss : 9.108783521337854e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0011770565169297305\n",
      "Best NLL Loss : 3.140253785763985e-06\n",
      "Best MSE Loss : 1.2561015143055937e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001153146353006767\n",
      "Best NLL Loss : 2.9238899598994255e-06\n",
      "Best MSE Loss : 1.1695559839597702e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0013274264422784437\n",
      "Best NLL Loss : 2.7121155720252488e-06\n",
      "Best MSE Loss : 1.0848462288100993e-06\n",
      "7\n",
      "Best ELBO Loss : 0.001087059343923039\n",
      "Best NLL Loss : 1.380750820341207e-06\n",
      "Best MSE Loss : 5.523003281364829e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0011957304578758775\n",
      "Best NLL Loss : 2.3829206217532967e-06\n",
      "Best MSE Loss : 9.531682487013188e-07\n",
      "9\n",
      "13\n",
      "902641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.0016599833965048403\n",
      "Best NLL Loss : 6.475329596426604e-05\n",
      "Best MSE Loss : 2.5901318385706422e-05\n",
      "0\n",
      "Best ELBO Loss : 0.0010053094677922277\n",
      "Best NLL Loss : 2.4351933635698704e-06\n",
      "Best MSE Loss : 9.740773454279484e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0011920657312675257\n",
      "Best NLL Loss : 2.4382435699843554e-06\n",
      "Best MSE Loss : 9.752974279937421e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0009054628823493258\n",
      "Best NLL Loss : 1.8314430712388828e-06\n",
      "Best MSE Loss : 7.325772284955531e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0010155632936788973\n",
      "Best NLL Loss : 2.8910758196890518e-06\n",
      "Best MSE Loss : 1.1564303278756208e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0009273360857196231\n",
      "Best NLL Loss : 1.8589529942661529e-06\n",
      "Best MSE Loss : 7.435811977064612e-07\n",
      "5\n",
      "Best ELBO Loss : 0.001207021762320913\n",
      "Best NLL Loss : 2.6634570530785923e-06\n",
      "Best MSE Loss : 1.0653828212314368e-06\n",
      "6\n",
      "Best ELBO Loss : 0.001257730879937737\n",
      "Best NLL Loss : 2.5603823157587696e-06\n",
      "Best MSE Loss : 1.024152926303508e-06\n",
      "7\n",
      "Best ELBO Loss : 0.001380322829189257\n",
      "Best NLL Loss : 2.8739427826200896e-06\n",
      "Best MSE Loss : 1.1495771130480357e-06\n",
      "8\n",
      "Best ELBO Loss : 0.001638308096274822\n",
      "Best NLL Loss : 3.5145215678352164e-06\n",
      "Best MSE Loss : 1.4058086271340864e-06\n",
      "9\n",
      "14\n",
      "897795\n",
      "Best ELBO Loss : 0.0017095996648283029\n",
      "Best NLL Loss : 5.358771566161926e-05\n",
      "Best MSE Loss : 2.1435086264647705e-05\n",
      "0\n",
      "Best ELBO Loss : 0.0012776792029466008\n",
      "Best NLL Loss : 2.402422829120193e-06\n",
      "Best MSE Loss : 9.609691316480771e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0011373508595942396\n",
      "Best NLL Loss : 1.7020650388476644e-06\n",
      "Best MSE Loss : 6.808260155390658e-07\n",
      "2\n",
      "Best ELBO Loss : 0.001086932363782829\n",
      "Best NLL Loss : 1.601175722668984e-06\n",
      "Best MSE Loss : 6.404702890675935e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0011985312236354852\n",
      "Best NLL Loss : 2.1049856843405834e-06\n",
      "Best MSE Loss : 8.419942737362333e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0010169576652123873\n",
      "Best NLL Loss : 2.582263885473211e-06\n",
      "Best MSE Loss : 1.0329055541892845e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001299117725210564\n",
      "Best NLL Loss : 1.4918829802488997e-06\n",
      "Best MSE Loss : 5.967531920995599e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0011445882671765437\n",
      "Best NLL Loss : 2.4301273336851135e-06\n",
      "Best MSE Loss : 9.720509334740453e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0009971569923293263\n",
      "Best NLL Loss : 1.6039184683677066e-06\n",
      "Best MSE Loss : 6.415673873470826e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0012933534521036118\n",
      "Best NLL Loss : 2.085817298645951e-06\n",
      "Best MSE Loss : 8.343269194583804e-07\n",
      "9\n",
      "15\n",
      "678150\n",
      "Best ELBO Loss : 0.0009311135271519529\n",
      "Best NLL Loss : 3.9912619279326534e-06\n",
      "Best MSE Loss : 1.596504771173061e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0009293993658128863\n",
      "Best NLL Loss : 1.3818841659416164e-06\n",
      "Best MSE Loss : 5.527536663766465e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0007637619104676751\n",
      "Best NLL Loss : 1.0046326964697894e-06\n",
      "Best MSE Loss : 4.0185307858791573e-07\n",
      "2\n",
      "Best ELBO Loss : 0.000943355313556497\n",
      "Best NLL Loss : 1.3473003358720726e-06\n",
      "Best MSE Loss : 5.389201343488292e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0008567296140748621\n",
      "Best NLL Loss : 1.213705838867817e-06\n",
      "Best MSE Loss : 4.854823355471267e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0008544165178971745\n",
      "Best NLL Loss : 1.2348438771320525e-06\n",
      "Best MSE Loss : 4.93937550852821e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0008102988774431643\n",
      "Best NLL Loss : 1.2724789926776203e-06\n",
      "Best MSE Loss : 5.08991597071048e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0009639857680384101\n",
      "Best NLL Loss : 1.3369496008063019e-06\n",
      "Best MSE Loss : 5.347798403225208e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0008342721661353598\n",
      "Best NLL Loss : 1.1245483861459456e-06\n",
      "Best MSE Loss : 4.4981935445837826e-07\n",
      "8\n",
      "Best ELBO Loss : 0.000954729898412958\n",
      "Best NLL Loss : 8.539374507485766e-07\n",
      "Best MSE Loss : 3.4157498029943067e-07\n",
      "9\n",
      "16\n",
      "361884\n",
      "Best ELBO Loss : 0.0011963408444191268\n",
      "Best NLL Loss : 2.94442878741003e-06\n",
      "Best MSE Loss : 1.177771514964012e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0010797879389508784\n",
      "Best NLL Loss : 1.970466281910328e-06\n",
      "Best MSE Loss : 7.88186512764131e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0009886374720230132\n",
      "Best NLL Loss : 1.3610908918627806e-06\n",
      "Best MSE Loss : 5.444363567451123e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0010221916430484322\n",
      "Best NLL Loss : 1.7092818626125274e-06\n",
      "Best MSE Loss : 6.837127450450111e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0011616383640563133\n",
      "Best NLL Loss : 2.4217519235417297e-06\n",
      "Best MSE Loss : 9.687007694166916e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0012171140234033117\n",
      "Best NLL Loss : 2.7926265724883767e-06\n",
      "Best MSE Loss : 1.1170506289953509e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0009136994158347315\n",
      "Best NLL Loss : 2.0375463655831753e-06\n",
      "Best MSE Loss : 8.1501854623327e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0008856966140945035\n",
      "Best NLL Loss : 1.4450603277385855e-06\n",
      "Best MSE Loss : 5.780241310954342e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0010470474558378431\n",
      "Best NLL Loss : 1.6076825088840006e-06\n",
      "Best MSE Loss : 6.430730035536001e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0015479260864462423\n",
      "Best NLL Loss : 3.864423718733114e-06\n",
      "Best MSE Loss : 1.5457694874932458e-06\n",
      "9\n",
      "17\n",
      "928159\n",
      "Best ELBO Loss : 0.0010589366606335903\n",
      "Best NLL Loss : 1.5054821759375355e-06\n",
      "Best MSE Loss : 6.021928703750143e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0009339117864844345\n",
      "Best NLL Loss : 8.409916470496906e-06\n",
      "Best MSE Loss : 3.3639665881987625e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0008954680249850076\n",
      "Best NLL Loss : 1.5040025346252486e-06\n",
      "Best MSE Loss : 6.016010138500994e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0008288312554231724\n",
      "Best NLL Loss : 1.6097022947639476e-06\n",
      "Best MSE Loss : 6.43880917905579e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0009306843593567192\n",
      "Best NLL Loss : 1.5218541983845378e-06\n",
      "Best MSE Loss : 6.087416793538153e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0011672105691435525\n",
      "Best NLL Loss : 1.5207709373586448e-06\n",
      "Best MSE Loss : 6.083083749434579e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0009779164264279785\n",
      "Best NLL Loss : 1.5192840623133863e-06\n",
      "Best MSE Loss : 6.077136249253546e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0008283598427239536\n",
      "Best NLL Loss : 1.0810461968769683e-06\n",
      "Best MSE Loss : 4.3241847875078733e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0009768372062370575\n",
      "Best NLL Loss : 1.4314931355360445e-06\n",
      "Best MSE Loss : 5.725972542144177e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0008540404625088942\n",
      "Best NLL Loss : 6.218980677295008e-07\n",
      "Best MSE Loss : 2.487592270918003e-07\n",
      "9\n",
      "18\n",
      "446640\n",
      "Best ELBO Loss : 0.0013659462992554737\n",
      "Best NLL Loss : 3.066984309809909e-06\n",
      "Best MSE Loss : 1.2267937239239635e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0019374994215765893\n",
      "Best NLL Loss : 4.739253608775534e-06\n",
      "Best MSE Loss : 1.8957014435102134e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0010177066240137718\n",
      "Best NLL Loss : 1.8234205026683397e-06\n",
      "Best MSE Loss : 7.29368201067336e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0018686659908765532\n",
      "Best NLL Loss : 4.547480218039051e-06\n",
      "Best MSE Loss : 1.8189920872156203e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0022339641011928295\n",
      "Best NLL Loss : 6.119621568723364e-06\n",
      "Best MSE Loss : 2.4478486274893457e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0025742068937562325\n",
      "Best NLL Loss : 7.5769509573926586e-06\n",
      "Best MSE Loss : 3.0307803829570634e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001332381718190505\n",
      "Best NLL Loss : 2.25226383942784e-06\n",
      "Best MSE Loss : 9.009055357711358e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0017223715513210685\n",
      "Best NLL Loss : 3.93124784636568e-06\n",
      "Best MSE Loss : 1.572499138546272e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0010779604259408978\n",
      "Best NLL Loss : 1.3927184563847902e-06\n",
      "Best MSE Loss : 5.570873825539159e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0013233911962299593\n",
      "Best NLL Loss : 3.1884912624368012e-06\n",
      "Best MSE Loss : 1.2753965049747205e-06\n",
      "9\n",
      "19\n",
      "26580\n",
      "Best ELBO Loss : 0.001011380280322351\n",
      "Best NLL Loss : 6.845173282658997e-06\n",
      "Best MSE Loss : 2.7380693130635983e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0009587076220918171\n",
      "Best NLL Loss : 3.968313445697822e-06\n",
      "Best MSE Loss : 1.587325378279129e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0010219206557340681\n",
      "Best NLL Loss : 1.619901783271886e-06\n",
      "Best MSE Loss : 6.479607133087543e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0011002393344408915\n",
      "Best NLL Loss : 1.3786406191430555e-06\n",
      "Best MSE Loss : 5.514562476572221e-07\n",
      "3\n",
      "Best ELBO Loss : 0.001129621747087878\n",
      "Best NLL Loss : 1.7340280301743449e-06\n",
      "Best MSE Loss : 6.936112120697378e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0011634246216849593\n",
      "Best NLL Loss : 1.568451290401222e-06\n",
      "Best MSE Loss : 6.273805161604887e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0010933695922917433\n",
      "Best NLL Loss : 2.186131811450849e-06\n",
      "Best MSE Loss : 8.744527245803394e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0009335091411521273\n",
      "Best NLL Loss : 1.3646612241152545e-06\n",
      "Best MSE Loss : 5.458644896461018e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0010955816036057088\n",
      "Best NLL Loss : 1.7800541952953944e-06\n",
      "Best MSE Loss : 7.120216781181576e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0012094089472135157\n",
      "Best NLL Loss : 1.962603906899747e-06\n",
      "Best MSE Loss : 7.850415627598987e-07\n",
      "9\n",
      "20\n",
      "865407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.0009592770675668955\n",
      "Best NLL Loss : 5.9613013631215644e-06\n",
      "Best MSE Loss : 2.384520545248626e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0009792510253032077\n",
      "Best NLL Loss : 3.441391915024105e-06\n",
      "Best MSE Loss : 1.3765567660096422e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0008842240479743508\n",
      "Best NLL Loss : 1.6734418089293078e-06\n",
      "Best MSE Loss : 6.69376723571723e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0012782139739918646\n",
      "Best NLL Loss : 2.8006874476336052e-06\n",
      "Best MSE Loss : 1.1202749790534421e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0011145796214860892\n",
      "Best NLL Loss : 5.408960986010305e-06\n",
      "Best MSE Loss : 2.1635843944041222e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0011877245559367186\n",
      "Best NLL Loss : 6.131416598435953e-06\n",
      "Best MSE Loss : 2.452566639374381e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0010240550598489368\n",
      "Best NLL Loss : 1.5410034656207849e-06\n",
      "Best MSE Loss : 6.164013862483139e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0007866130429448031\n",
      "Best NLL Loss : 2.3257278894476365e-06\n",
      "Best MSE Loss : 9.302911557790544e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0010101121492100981\n",
      "Best NLL Loss : 4.916234221056917e-06\n",
      "Best MSE Loss : 1.9664936884227665e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0010232824165181823\n",
      "Best NLL Loss : 1.971850464170701e-06\n",
      "Best MSE Loss : 7.887401856682802e-07\n",
      "9\n",
      "21\n",
      "789523\n",
      "Best ELBO Loss : 0.0016847428269757346\n",
      "Best NLL Loss : 1.9306531192795423e-05\n",
      "Best MSE Loss : 7.72261247711817e-06\n",
      "0\n",
      "Best ELBO Loss : 0.001696140826342195\n",
      "Best NLL Loss : 3.452953947310505e-06\n",
      "Best MSE Loss : 1.381181578924202e-06\n",
      "1\n",
      "Best ELBO Loss : 0.001253357204739525\n",
      "Best NLL Loss : 2.2143114791788933e-06\n",
      "Best MSE Loss : 8.857245916715573e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0016120492798214695\n",
      "Best NLL Loss : 3.7818965849767497e-06\n",
      "Best MSE Loss : 1.5127586339907e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0017599943015872796\n",
      "Best NLL Loss : 3.5593484530787443e-06\n",
      "Best MSE Loss : 1.4237393812314975e-06\n",
      "4\n",
      "Best ELBO Loss : 0.001590838196605953\n",
      "Best NLL Loss : 1.0587967237444605e-05\n",
      "Best MSE Loss : 4.235186894977843e-06\n",
      "5\n",
      "Best ELBO Loss : 0.001404186431521614\n",
      "Best NLL Loss : 2.5512224694730225e-06\n",
      "Best MSE Loss : 1.020488987789209e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0014307514080382402\n",
      "Best NLL Loss : 2.6298502710677124e-06\n",
      "Best MSE Loss : 1.0519401084270853e-06\n",
      "7\n",
      "Best ELBO Loss : 0.001638101264435571\n",
      "Best NLL Loss : 2.165359359107077e-06\n",
      "Best MSE Loss : 8.661437436428307e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0017994773225421224\n",
      "Best NLL Loss : 3.6383272813725333e-06\n",
      "Best MSE Loss : 1.4553309125490133e-06\n",
      "9\n",
      "22\n",
      "704840\n",
      "Best ELBO Loss : 0.0015236309070381382\n",
      "Best NLL Loss : 2.4038250781827163e-06\n",
      "Best MSE Loss : 9.615300312730864e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0013931451953134457\n",
      "Best NLL Loss : 2.4227554761599005e-06\n",
      "Best MSE Loss : 9.691021904639603e-07\n",
      "1\n",
      "Best ELBO Loss : 0.0012544184271272073\n",
      "Best NLL Loss : 2.113076994260714e-06\n",
      "Best MSE Loss : 8.452307977042856e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0014975962948518403\n",
      "Best NLL Loss : 2.710729637638644e-06\n",
      "Best MSE Loss : 1.0842918550554577e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0014346008140576265\n",
      "Best NLL Loss : 2.3520480060702054e-06\n",
      "Best MSE Loss : 9.40819202428082e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0013527254406278792\n",
      "Best NLL Loss : 2.162456796355266e-06\n",
      "Best MSE Loss : 8.649827185421065e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0017385012763076876\n",
      "Best NLL Loss : 3.0365532212400126e-06\n",
      "Best MSE Loss : 1.214621288496005e-06\n",
      "6\n",
      "Best ELBO Loss : 0.001417700255903954\n",
      "Best NLL Loss : 2.384251124156927e-06\n",
      "Best MSE Loss : 9.53700449662771e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0015676546824492973\n",
      "Best NLL Loss : 2.6243330422875323e-06\n",
      "Best MSE Loss : 1.0497332169150131e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0015003646063619384\n",
      "Best NLL Loss : 2.4942981791149308e-06\n",
      "Best MSE Loss : 9.977192716459722e-07\n",
      "9\n",
      "23\n",
      "359269\n",
      "Best ELBO Loss : 0.001818101340166948\n",
      "Best NLL Loss : 3.0495704699967557e-06\n",
      "Best MSE Loss : 1.219828187998702e-06\n",
      "0\n",
      "Best ELBO Loss : 0.00153719319614734\n",
      "Best NLL Loss : 4.169960049096934e-06\n",
      "Best MSE Loss : 1.6679840196387735e-06\n",
      "1\n",
      "Best ELBO Loss : 0.001607936427463263\n",
      "Best NLL Loss : 2.6399419515098183e-06\n",
      "Best MSE Loss : 1.0559767806039273e-06\n",
      "2\n",
      "Best ELBO Loss : 0.001478915445575685\n",
      "Best NLL Loss : 4.012624480963422e-06\n",
      "Best MSE Loss : 1.6050497923853687e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0017532833662828396\n",
      "Best NLL Loss : 4.535588287996624e-06\n",
      "Best MSE Loss : 1.8142353151986496e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0019281322395155415\n",
      "Best NLL Loss : 4.755492415469526e-06\n",
      "Best MSE Loss : 1.9021969661878098e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0014040792903173406\n",
      "Best NLL Loss : 3.6152298852327714e-06\n",
      "Best MSE Loss : 1.4460919540931089e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0019184511477245798\n",
      "Best NLL Loss : 5.0980718848266606e-06\n",
      "Best MSE Loss : 2.0392287539306642e-06\n",
      "7\n",
      "Best ELBO Loss : 0.001235279917174895\n",
      "Best NLL Loss : 1.669486885028037e-06\n",
      "Best MSE Loss : 6.677947540112148e-07\n",
      "8\n",
      "Best ELBO Loss : 0.001636215254508892\n",
      "Best NLL Loss : 4.561124992389672e-06\n",
      "Best MSE Loss : 1.8244499969558686e-06\n",
      "9\n",
      "24\n",
      "38248\n",
      "Best ELBO Loss : 0.001214138192161584\n",
      "Best NLL Loss : 2.033843942565105e-06\n",
      "Best MSE Loss : 8.135375770260419e-07\n",
      "0\n",
      "Best ELBO Loss : 0.001315088664803363\n",
      "Best NLL Loss : 2.3276665036221706e-06\n",
      "Best MSE Loss : 9.310666014488684e-07\n",
      "1\n",
      "Best ELBO Loss : 0.001171490076446949\n",
      "Best NLL Loss : 1.9181119140235947e-06\n",
      "Best MSE Loss : 7.672447656094379e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0013716507166644382\n",
      "Best NLL Loss : 2.167382146783539e-06\n",
      "Best MSE Loss : 8.669528587134155e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0011355011110641704\n",
      "Best NLL Loss : 1.5963549558866085e-06\n",
      "Best MSE Loss : 6.385419823546433e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0012429000532138913\n",
      "Best NLL Loss : 2.16877326842724e-06\n",
      "Best MSE Loss : 8.67509307370896e-07\n",
      "5\n",
      "Best ELBO Loss : 0.001636200885127199\n",
      "Best NLL Loss : 2.273424385617504e-06\n",
      "Best MSE Loss : 9.093697542470015e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0010515793128678067\n",
      "Best NLL Loss : 1.8958335086572454e-06\n",
      "Best MSE Loss : 7.583334034628982e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0012768862760393364\n",
      "Best NLL Loss : 2.1591081047067583e-06\n",
      "Best MSE Loss : 8.636432418827033e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0011573642282272914\n",
      "Best NLL Loss : 1.921507302123251e-06\n",
      "Best MSE Loss : 7.686029208493006e-07\n",
      "9\n",
      "25\n",
      "809111\n",
      "Best ELBO Loss : 0.0008001762630768086\n",
      "Best NLL Loss : 3.431144130767611e-07\n",
      "Best MSE Loss : 1.3724576523070446e-07\n",
      "0\n",
      "Best ELBO Loss : 0.0010948069731821386\n",
      "Best NLL Loss : 1.8169289706905305e-06\n",
      "Best MSE Loss : 7.267715882762121e-07\n",
      "1\n",
      "Best ELBO Loss : 0.001159428987531044\n",
      "Best NLL Loss : 1.7461873331588447e-06\n",
      "Best MSE Loss : 6.98474933263538e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0010932999878630802\n",
      "Best NLL Loss : 1.7050194925023663e-06\n",
      "Best MSE Loss : 6.820077970009464e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0010719534156716048\n",
      "Best NLL Loss : 1.7414575695172658e-06\n",
      "Best MSE Loss : 6.965830278069062e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0009782326958386327\n",
      "Best NLL Loss : 1.210559895569862e-06\n",
      "Best MSE Loss : 4.842239582279448e-07\n",
      "5\n",
      "Best ELBO Loss : 0.0009784497079398588\n",
      "Best NLL Loss : 1.5389327533457312e-06\n",
      "Best MSE Loss : 6.155731013382926e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0009992443089194688\n",
      "Best NLL Loss : 1.6368298403248032e-06\n",
      "Best MSE Loss : 6.547319361299211e-07\n",
      "7\n",
      "Best ELBO Loss : 0.000939816929662729\n",
      "Best NLL Loss : 9.163470199819682e-07\n",
      "Best MSE Loss : 3.6653880799278727e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0009193403643302824\n",
      "Best NLL Loss : 1.356245832888759e-06\n",
      "Best MSE Loss : 5.424983331555036e-07\n",
      "9\n",
      "26\n",
      "137636\n",
      "Best ELBO Loss : 0.0015431955419389677\n",
      "Best NLL Loss : 2.7830579064393065e-06\n",
      "Best MSE Loss : 1.1132231625757227e-06\n",
      "0\n",
      "Best ELBO Loss : 0.00192903268374098\n",
      "Best NLL Loss : 4.50973583750081e-06\n",
      "Best MSE Loss : 1.8038943350003239e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0010663508472422724\n",
      "Best NLL Loss : 1.1896333762821216e-06\n",
      "Best MSE Loss : 4.758533505128486e-07\n",
      "2\n",
      "Best ELBO Loss : 0.0009098892304333513\n",
      "Best NLL Loss : 1.9525431029093384e-06\n",
      "Best MSE Loss : 7.810172411637354e-07\n",
      "3\n",
      "Best ELBO Loss : 0.0010382704535428237\n",
      "Best NLL Loss : 1.1981144894321367e-06\n",
      "Best MSE Loss : 4.792457957728547e-07\n",
      "4\n",
      "Best ELBO Loss : 0.0015313110674870004\n",
      "Best NLL Loss : 3.1776866162972817e-06\n",
      "Best MSE Loss : 1.271074646518913e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0013481518245523046\n",
      "Best NLL Loss : 2.4397152791894086e-06\n",
      "Best MSE Loss : 9.758861116757637e-07\n",
      "6\n",
      "Best ELBO Loss : 0.0011758895301883477\n",
      "Best NLL Loss : 2.248378428501552e-06\n",
      "Best MSE Loss : 8.993513714006209e-07\n",
      "7\n",
      "Best ELBO Loss : 0.0007701116266398121\n",
      "Best NLL Loss : 6.558093106070328e-07\n",
      "Best MSE Loss : 2.623237242428131e-07\n",
      "8\n",
      "Best ELBO Loss : 0.0014553337269759142\n",
      "Best NLL Loss : 1.1224627381991626e-06\n",
      "Best MSE Loss : 4.489850952796649e-07\n",
      "9\n",
      "27\n",
      "698530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.0016773360914654797\n",
      "Best NLL Loss : 1.6368032682953366e-06\n",
      "Best MSE Loss : 6.547213073181347e-07\n",
      "0\n",
      "Best ELBO Loss : 0.001557329072472253\n",
      "Best NLL Loss : 3.5534041302127947e-06\n",
      "Best MSE Loss : 1.4213616520851176e-06\n",
      "1\n",
      "Best ELBO Loss : 0.001688858829265599\n",
      "Best NLL Loss : 3.850505708667782e-06\n",
      "Best MSE Loss : 1.540202283467113e-06\n",
      "2\n",
      "Best ELBO Loss : 0.0022393420291789722\n",
      "Best NLL Loss : 5.272190210620914e-06\n",
      "Best MSE Loss : 2.108876084248365e-06\n",
      "3\n",
      "Best ELBO Loss : 0.002685215266757586\n",
      "Best NLL Loss : 1.1898606255441731e-05\n",
      "Best MSE Loss : 4.759442502176693e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0015130906021870046\n",
      "Best NLL Loss : 4.776147187205148e-06\n",
      "Best MSE Loss : 1.9104588748820593e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0015171075155714986\n",
      "Best NLL Loss : 4.926532508573407e-06\n",
      "Best MSE Loss : 1.9706130034293628e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0017063244584679764\n",
      "Best NLL Loss : 3.4343657070828257e-06\n",
      "Best MSE Loss : 1.3737462828331306e-06\n",
      "7\n",
      "Best ELBO Loss : 0.0026364742017745075\n",
      "Best NLL Loss : 1.3362427984285016e-05\n",
      "Best MSE Loss : 5.344971193714007e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0019830032008464928\n",
      "Best NLL Loss : 4.540199140304555e-06\n",
      "Best MSE Loss : 1.8160796561218221e-06\n",
      "9\n",
      "28\n",
      "230177\n",
      "Best ELBO Loss : 0.0013303004909578165\n",
      "Best NLL Loss : 4.0186826683814126e-06\n",
      "Best MSE Loss : 1.6074730673525649e-06\n",
      "0\n",
      "Best ELBO Loss : 0.0013893501419254008\n",
      "Best NLL Loss : 7.835810745096862e-06\n",
      "Best MSE Loss : 3.134324298038746e-06\n",
      "1\n",
      "Best ELBO Loss : 0.0013705798466900575\n",
      "Best NLL Loss : 2.496963251579536e-06\n",
      "Best MSE Loss : 9.987853006318143e-07\n",
      "2\n",
      "Best ELBO Loss : 0.002415125383511893\n",
      "Best NLL Loss : 6.322985398281534e-06\n",
      "Best MSE Loss : 2.529194159312613e-06\n",
      "3\n",
      "Best ELBO Loss : 0.0015944943827646716\n",
      "Best NLL Loss : 3.559794024778537e-06\n",
      "Best MSE Loss : 1.423917609911415e-06\n",
      "4\n",
      "Best ELBO Loss : 0.0015408057869234209\n",
      "Best NLL Loss : 4.2231012686640774e-06\n",
      "Best MSE Loss : 1.6892405074656313e-06\n",
      "5\n",
      "Best ELBO Loss : 0.0014359606368169742\n",
      "Best NLL Loss : 2.884425944696597e-06\n",
      "Best MSE Loss : 1.1537703778786386e-06\n",
      "6\n",
      "Best ELBO Loss : 0.0015271997043090398\n",
      "Best NLL Loss : 2.408082663279747e-06\n",
      "Best MSE Loss : 9.632330653118986e-07\n",
      "7\n",
      "Best ELBO Loss : 0.001651604807152483\n",
      "Best NLL Loss : 2.917241472382555e-06\n",
      "Best MSE Loss : 1.166896588953022e-06\n",
      "8\n",
      "Best ELBO Loss : 0.0014554613337295537\n",
      "Best NLL Loss : 1.7425291569226167e-05\n",
      "Best MSE Loss : 6.970116627690468e-06\n",
      "9\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "n = 30 # The number of samples of data.\n",
    "d = 5 # The number of variables in data.\n",
    "x_dims = 1 # The number of input dimensions: default 1.\n",
    "z_dims = d # The number of latent variable dimensions: default the same as variable size.\n",
    "epochs = 200 # Number of epochs to train.\n",
    "batch_size = 10 # Number of samples per batch. note: should be divisible by sample size, otherwise throw an error.\n",
    "n_var=5\n",
    "\n",
    "n_times=30 #no. of replicates\n",
    "time_stamp=10 #no. of timestamp\n",
    "np.random.seed(1234567) #Random seed\n",
    "seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "average_coef_list=np.zeros((n_times,time_stamp,n_var,n_var))\n",
    "B_list=np.zeros((n_times,d, d))\n",
    "FDR_total=[]\n",
    "TPR_total=[]\n",
    "SHD_total=[]\n",
    "time_list=[]\n",
    "for replicate in range(n_times):\n",
    "  seed=seed_list[replicate]\n",
    "  print(seed)\n",
    "  X_all=data_create(seed,time_stamp) #create data\n",
    "  average_list=np.zeros((time_stamp,d, d))\n",
    "  FDR_list_piece=[]\n",
    "  TPR_list_piece=[]\n",
    "  SHD_list_piece=[]\n",
    "  base_DAG=np.zeros((5, 5))\n",
    "  ####estimate at each time_stamp####\n",
    "  timestart=time.time()\n",
    "  for j in range(time_stamp):\n",
    "  # ----------- Configurations:\n",
    "      k_max_iter = int(1e2) # The max iteration number for searching parameters.\n",
    "      original_lr = 3e-3  # Initial learning rate.\n",
    "      encoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "      decoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "      temp = 0.5 # Temperature for Gumbel softmax.\n",
    "      factor = True # Factor graph model.\n",
    "      encoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "      decoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "      tau_B = 0. # Coefficient for L-1 norm of matrix B.\n",
    "      lambda1 = 0. # Coefficient for DAG constraint h1(B).\n",
    "      lambda2 = 0. # Coefficient for identification constraint h2(B).\n",
    "      c_B = 1 # Coefficient for absolute value h1(B).\n",
    "      d_B = 1 # Coefficient for absolute value h2(B).\n",
    "      h1_tol = 1e-8 # The tolerance of error of h1(B) to zero.\n",
    "      h2_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "      lr_decay = 200 # After how many epochs to decay LR by a factor of gamma. \n",
    "      gamma = 1.0 # LR decay factor. \n",
    "      ######################\n",
    "\n",
    "\n",
    "      X=X_all[(j*30):(j*30+30),:]\n",
    "\n",
    "\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "      torch.manual_seed(seed)\n",
    "      feat_train = torch.FloatTensor(X)\n",
    "      feat_valid = torch.FloatTensor(X)\n",
    "      feat_test = torch.FloatTensor(X)\n",
    "\n",
    "      # Reconstruct itself\n",
    "      train_data = TensorDataset(feat_train, feat_train)\n",
    "      valid_data = TensorDataset(feat_valid, feat_train)\n",
    "      test_data = TensorDataset(feat_test, feat_train)\n",
    "\n",
    "      train_loader = DataLoader(train_data, batch_size = batch_size)\n",
    "      valid_loader = DataLoader(valid_data, batch_size = batch_size)\n",
    "      test_loader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "      # ----------- Load modules:\n",
    "      off_diag = np.ones([d, d]) - np.eye(d) # Generate off-diagonal interaction graph\n",
    "      rel_rec = np.array(encode_onehot(np.where(off_diag)[1]), dtype = np.float64)\n",
    "      rel_send = np.array(encode_onehot(np.where(off_diag)[0]), dtype = np.float64)\n",
    "      rel_rec = torch.DoubleTensor(rel_rec)\n",
    "      rel_send = torch.DoubleTensor(rel_send)\n",
    "      adj_A = np.zeros((d, d)) # Add adjacency matrix\n",
    "\n",
    "      encoder = MLPEncoder(d * x_dims, x_dims, encoder_hidden,\n",
    "                              int(z_dims), adj_A,\n",
    "                              batch_size = batch_size,\n",
    "                              do_prob = encoder_dropout, factor = factor).double()\n",
    "      decoder = MLPDecoder(d * x_dims,\n",
    "                              z_dims, x_dims, encoder,\n",
    "                              data_variable_size = d,\n",
    "                              batch_size = batch_size,\n",
    "                              n_hid=decoder_hidden,\n",
    "                              do_prob=decoder_dropout).double()\n",
    "\n",
    "      # ----------- Set up optimizer:\n",
    "      optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr = original_lr)\n",
    "      scheduler = lr_scheduler.StepLR(optimizer, step_size = lr_decay,\n",
    "                                      gamma = gamma)\n",
    "\n",
    "      rel_rec = Variable(rel_rec)\n",
    "      rel_send = Variable(rel_send)\n",
    "\n",
    "      # ----------- Main:\n",
    "      best_ELBO_loss = np.inf\n",
    "      best_NLL_loss = np.inf\n",
    "      best_MSE_loss = np.inf\n",
    "      h1_B_new = torch.tensor(1.)\n",
    "      h2_B_new = 1\n",
    "      h1_B_old = np.inf\n",
    "      h2_B_old = np.inf\n",
    "      lr = original_lr\n",
    "\n",
    "      try:\n",
    "          for step_k in range(k_max_iter):\n",
    "              while c_B * d_B < 1e+20:\n",
    "                  for epoch in range(epochs):\n",
    "                      old_lr = lr \n",
    "                      ELBO_loss, NLL_loss, MSE_loss, origin_B, optimizer, lr = train(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr)\n",
    "\n",
    "                      if ELBO_loss < best_ELBO_loss:\n",
    "                          best_ELBO_loss = ELBO_loss\n",
    "\n",
    "                      if NLL_loss < best_NLL_loss:\n",
    "                          best_NLL_loss = NLL_loss\n",
    "\n",
    "                      if MSE_loss < best_MSE_loss:\n",
    "                          best_MSE_loss = MSE_loss\n",
    "\n",
    "                  if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                      break\n",
    "\n",
    "                  # Update parameters\n",
    "                  B_new = origin_B.data.clone()\n",
    "                  h1_B_new = fun_h1_B(B_new)\n",
    "                  h2_B_new = fun_h2_B(B_new)\n",
    "                  if h1_B_new.item() > 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                      c_B *= 10\n",
    "                      d_B *= 10\n",
    "                  elif h1_B_new.item() > 0.25 * h1_B_old and h2_B_new < 0.25 * h2_B_old:\n",
    "                      c_B *= 10\n",
    "                  elif h1_B_new.item() < 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                      d_B *= 10\n",
    "                  else:\n",
    "                      break\n",
    "\n",
    "              # Update parameters    \n",
    "              h1_B_old = h1_B_new.item()\n",
    "              h2_B_old = h2_B_new\n",
    "              lambda1 += c_B * h1_B_new.item()\n",
    "              lambda2 += d_B * h2_B_new\n",
    "\n",
    "              if h1_B_new.item() <= h1_tol and h2_B_new <= h2_tol:\n",
    "                  break\n",
    "\n",
    "      except KeyboardInterrupt:\n",
    "          print('KeyboardInterrupt')\n",
    "\n",
    "      predB = np.matrix(origin_B.data.clone().numpy())\n",
    "      print('Best ELBO Loss :', best_ELBO_loss)\n",
    "      print('Best NLL Loss :', best_NLL_loss)\n",
    "      print('Best MSE Loss :', best_MSE_loss)\n",
    "      #calculate_effect(predB)\n",
    "      print(j)\n",
    "      average_list[j,:,:]=predB\n",
    "      #FDR, TPR, SHD\n",
    "      base_DAG[0,4]=cos(j)\n",
    "      base_graph=nx.from_numpy_matrix(base_DAG,create_using=nx.DiGraph)\n",
    "      a=abs(predB)\n",
    "      a[a<0.4] = 0\n",
    "      base_estimate=nx.from_numpy_matrix(a,create_using=nx.DiGraph)\n",
    "      FDR,TPR,SHD=count_accuracy(base_graph,base_estimate)\n",
    "      FDR_list_piece.append(FDR)\n",
    "      TPR_list_piece.append(TPR)\n",
    "      SHD_list_piece.append(SHD)\n",
    "\n",
    "\n",
    "\n",
    "  average_coef_list[replicate,:,:,:]=average_list #average coef save to matrix\n",
    "  np.save(\"quadra_10_30_ANOCA\",average_coef_list)\n",
    "  FDR_total.append(mean(FDR_list_piece))\n",
    "  TPR_total.append(mean(TPR_list_piece))\n",
    "  SHD_total.append(mean(SHD_list_piece))\n",
    "  timeend=time.time()\n",
    "  time_list.append(timeend-timestart)\n",
    "  #####write at every epoch\n",
    "  df = pd.DataFrame(columns=('FDR', 'TPR',\"SHD\",\"time\"))\n",
    "  df[\"FDR\"]=FDR_total\n",
    "  df[\"TPR\"]=TPR_total\n",
    "  df[\"SHD\"]=SHD_total\n",
    "  df[\"time\"]=time_list\n",
    "  #df.to_csv(\"cos_rep10.csv\")\n",
    "  print(replicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "JZQDzmjXuLQj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "JZQDzmjXuLQj",
    "outputId": "0dcd2d5d-c32d-4961-87b4-1ee1042a1f86"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>SHD</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>718.534061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.45</td>\n",
       "      <td>672.521675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>725.070246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.35</td>\n",
       "      <td>716.542320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>724.934847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>748.936342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>765.518650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>743.192097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.20</td>\n",
       "      <td>739.361321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>727.557208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>753.956553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>780.090053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>731.089245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>717.458791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>734.780630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>686.821930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.85</td>\n",
       "      <td>727.646949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>764.195709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>734.905666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>694.056903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FDR   TPR   SHD        time\n",
       "0   0.258333  0.90  0.75  718.534061\n",
       "1   0.337500  0.65  1.45  672.521675\n",
       "2   0.058333  0.65  0.50  725.070246\n",
       "3   0.337500  0.65  1.35  716.542320\n",
       "4   0.075000  0.65  0.45  724.934847\n",
       "5   0.000000  0.65  0.35  748.936342\n",
       "6   0.133333  0.65  0.60  765.518650\n",
       "7   0.087500  0.75  0.50  743.192097\n",
       "8   0.283333  0.70  1.20  739.361321\n",
       "9   0.204167  0.65  0.80  727.557208\n",
       "10  0.050000  0.65  0.45  753.956553\n",
       "11  0.050000  0.65  0.45  780.090053\n",
       "12  0.075000  0.65  0.50  731.089245\n",
       "13  0.000000  0.65  0.35  717.458791\n",
       "14  0.062500  0.65  0.55  734.780630\n",
       "15  0.000000  0.65  0.35  686.821930\n",
       "16  0.200000  0.65  0.85  727.646949\n",
       "17  0.000000  0.65  0.35  764.195709\n",
       "18  0.133333  0.65  0.70  734.905666\n",
       "19  0.087500  0.65  0.60  694.056903"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=('FDR', 'TPR',\"SHD\",\"time\"))\n",
    "df[\"FDR\"]=FDR_total\n",
    "df[\"TPR\"]=TPR_total\n",
    "df[\"SHD\"]=SHD_total\n",
    "df[\"time\"]=time_list\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0ed51a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDR       0.121667\n",
       "TPR       0.670000\n",
       "SHD       0.655000\n",
       "time    730.358560\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adbdedc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_coef_list[:,:,0,4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dde9a6a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "dde9a6a8",
    "outputId": "656697fc-10a4-44c3-f962-830db0e1649f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(time_stamp),np.mean(average_coef_list[:,:,0,4],0))\n",
    "plt.plot(range(time_stamp),[cos(i) for i in range(time_stamp)])\n",
    "# Function add a legend  \n",
    "plt.legend([\"Estimated coeffcient\", \"True coefficient\"])\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6be67",
   "metadata": {
    "id": "42b6be67"
   },
   "source": [
    "# new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42046e3c",
   "metadata": {
    "id": "42046e3c"
   },
   "outputs": [],
   "source": [
    "# ----------- Configurations:\n",
    "#time_stamp=10\n",
    "n_timestamp=time_stamp\n",
    "sample__time=30\n",
    "n = time_stamp*sample__time # The number of samples of data.\n",
    "n_var = 5 # The number of variables in data.\n",
    "x_dims = 1 # The number of input dimensions: default 1.\n",
    "z_dims = d # The number of latent variable dimensions: default the same as variable size.\n",
    "epochs = 200 # Number of epochs to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7871866b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7871866b",
    "outputId": "95d27f29-9ad4-41bb-a565-112fc3b458d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b4a5750",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4a5750",
    "outputId": "c445bb2c-a8f6-47b5-b509-6845875ddd96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample__time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d0bc0a6",
   "metadata": {
    "id": "2d0bc0a6"
   },
   "outputs": [],
   "source": [
    "def create_X(X):\n",
    "    X_sum=np.zeros((sample__time, n_var,1))\n",
    "    for i in range(0,n_timestamp):\n",
    "        X_sum=X_sum+X[(sample__time*i):(sample__time*(i+1)),:] #sum i\n",
    "        print((sample__time*i),(sample__time*(i+1)))\n",
    "    return(X_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93a046a5",
   "metadata": {
    "id": "93a046a5"
   },
   "outputs": [],
   "source": [
    "def create_D(X,spline_list):\n",
    "    D_all=np.zeros((sample__time, n_var*len(spline_list),1))\n",
    "    for j in range(len(spline_list)):\n",
    "        D_sum=np.zeros((sample__time, n_var,1))\n",
    "        for i in range(0,n_timestamp):\n",
    "            D_sum=D_sum+X[(sample__time*i):(sample__time*(i+1)),:]*spline_list[j](i)#sum X_i *f(i)\n",
    "        ##horizontally append\n",
    "        D_all[:,n_var*j:n_var*(j+1),:]=D_sum\n",
    "        #print((n_features*j,n_features*(j+1)))\n",
    "    return(D_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "000b7448",
   "metadata": {
    "id": "000b7448"
   },
   "outputs": [],
   "source": [
    "def spl_const(x):\n",
    "    return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5880a65f",
   "metadata": {
    "id": "5880a65f"
   },
   "outputs": [],
   "source": [
    "#spline_list=[spl_const]\n",
    "spline_list=[spl1,spl2,spl3,spl4,spl5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bdb6397",
   "metadata": {
    "id": "9bdb6397"
   },
   "outputs": [],
   "source": [
    "def spl_consraint(spline_list,t,p):\n",
    "    k=len(spline_list)\n",
    "    output=torch.zeros(((p*k), p))\n",
    "    identity=torch.zeros((p, p*k))\n",
    "    for i in range(k):\n",
    "        output[(i*p):(i*p+p),:]=torch.ones((p,p))*spline_list[i](t).item() #gamma times basis\n",
    "        identity[:,(i*p):(i*p+p)]=torch.eye(p) #stacked identity matrix\n",
    "    #final=torch.matmul(identity,output)\n",
    "    return output, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "985dc159",
   "metadata": {
    "id": "985dc159"
   },
   "outputs": [],
   "source": [
    "# ----------- Configurations:\n",
    "n = 30 # The number of samples of data.\n",
    "d = 30 # The number of variables in data after spline.\n",
    "x_dims = 1 # The number of input dimensions: default 1.\n",
    "z_dims = d # The number of latent variable dimensions: default the same as variable size.\n",
    "epochs = 200 # Number of epochs to train.\n",
    "batch_size = 10 # Number of samples per batch. note: should be divisible by sample size, otherwise throw an error.\n",
    "k_max_iter = int(1e2) # The max iteration number for searching parameters.\n",
    "original_lr = 3e-3  # Initial learning rate.\n",
    "encoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "decoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "temp = 0.5 # Temperature for Gumbel softmax.\n",
    "factor = True # Factor graph model.\n",
    "encoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "decoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "tau_B = 0. # Coefficient for L-1 norm of matrix B.\n",
    "lambda1 = 0. # Coefficient for DAG constraint h1(B).\n",
    "lambda2 = 0. # Coefficient for identification constraint h2(B).\n",
    "c_B = 1 # Coefficient for absolute value h1(B).\n",
    "d_B = 1 # Coefficient for absolute value h2(B).\n",
    "e_B = 1 # Coefficient for absolute value h3(B)\n",
    "h1_tol = 1e-8 # The tolerance of error of h1(B) to zero.\n",
    "h2_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "h3_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "lr_decay = 200 # After how many epochs to decay LR by a factor of gamma. \n",
    "gamma = 1.0 # LR decay factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c588ed1",
   "metadata": {
    "id": "4c588ed1"
   },
   "outputs": [],
   "source": [
    "def fun_h2_B_new(B):\n",
    "    '''compute constraint h2(B) value'''\n",
    "    d = B.shape[0]\n",
    "    gamma=B[p:,:p]\n",
    "    h3_B = sum(sum(abs(B[:p, :])))+sum(abs(gamma[:, 0])) # uppper 0 and 0 column and every p-1,2p-1 row\n",
    "    for i in range(k):\n",
    "        h3_B=h3_B+sum(abs(gamma[((i+1)*p-1), 1:]))\n",
    "    return h3_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "705f6778",
   "metadata": {
    "id": "705f6778"
   },
   "outputs": [],
   "source": [
    "def fun_h1_B(B):\n",
    "    '''compute constraint h1(B) value'''\n",
    "    d = B.shape[0]\n",
    "    expm_B = matrix_poly(B * B, d)\n",
    "    h1_B = torch.trace(expm_B) - d\n",
    "    return h1_B.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "feedc251",
   "metadata": {
    "id": "feedc251"
   },
   "outputs": [],
   "source": [
    "def train_new(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr,p,k):\n",
    "        \n",
    "        nll_train = []\n",
    "        kl_train = []\n",
    "        mse_train = []\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update optimizer\n",
    "        optimizer, lr = update_optimizer(optimizer, old_lr, c_B, d_B)\n",
    "\n",
    "        for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "            data, relations = Variable(data).double(), Variable(relations).double()\n",
    "            relations = relations.unsqueeze(2) # Reshape data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_x, logits, origin_B, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data, rel_rec, rel_send) \n",
    "            edges = logits # Logits is of size: [num_sims, z_dims]\n",
    "\n",
    "            dec_x, output, adj_A_tilt_decoder = decoder(data, edges, d * x_dims, rel_rec, rel_send, origin_B, adj_A_tilt_encoder, Wa)\n",
    "\n",
    "            if torch.sum(output != output):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            target = data\n",
    "            preds = output\n",
    "            variance = 0.\n",
    "            \n",
    "            # Compute constraint functions h1(B) and h2(B)\n",
    "            h1_B = fun_h1_B(origin_B[p:,p:]) #acyclity on G\n",
    "            for i in range(n_timestamp):\n",
    "                #aa=torch.matmul(spl_consraint(spline_list,i,p).type(torch.FloatTensor),origin_B[p:,:p].type(torch.FloatTensor))#acyclity on Gamma\n",
    "                #print(fun_h1_B(aa))\n",
    "                output, identity=spl_consraint(spline_list,i,p)\n",
    "                h1_B=h1_B+fun_h1_B(torch.matmul(identity.type(torch.FloatTensor),(output.type(torch.FloatTensor)*origin_B[p:,:p].type(torch.FloatTensor))))#acyclity on Gamma\n",
    "            h2_B = fun_h2_B_new(origin_B) ##handle the zero on gamma and top\n",
    "            # Reconstruction accuracy loss:\n",
    "            loss_nll = nll_gaussian(preds, target, variance)\n",
    "            # KL loss:\n",
    "            loss_kl = kl_gaussian(logits)\n",
    "            # ELBO loss:\n",
    "            loss = loss_kl + loss_nll\n",
    "            # Loss function:\n",
    "            loss += lambda1 * h1_B + 0.5 * c_B * h1_B * h1_B + lambda2 * h2_B + 0.5 * d_B * h2_B * h2_B + 100. * torch.trace(origin_B * origin_B)\n",
    "\n",
    "            loss.backward()\n",
    "            loss = optimizer.step()\n",
    "\n",
    "            myA.data = stau(myA.data, tau_B * lr)\n",
    "\n",
    "            if torch.sum(origin_B != origin_B):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            mse_train.append(F.mse_loss(preds, target).item())\n",
    "            nll_train.append(loss_nll.item())\n",
    "            kl_train.append(loss_kl.item())\n",
    "\n",
    "        return np.mean(np.mean(kl_train) + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), origin_B, optimizer, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af937222",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af937222",
    "outputId": "e80610dd-84e9-4b89-dce3-cc71ee027097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.07818287436407118\n",
      "Best NLL Loss : 0.00021279066160837757\n",
      "Best MSE Loss : 1.4186044107225167e-05\n",
      "0\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.0984414308999451\n",
      "Best NLL Loss : 0.00039596808298519425\n",
      "Best MSE Loss : 2.6397872199012946e-05\n",
      "1\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.08315984828255162\n",
      "Best NLL Loss : 0.00028355952984412513\n",
      "Best MSE Loss : 1.8903968656275005e-05\n",
      "2\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.12237789599855997\n",
      "Best NLL Loss : 0.0008859617627780455\n",
      "Best MSE Loss : 5.906411751853639e-05\n",
      "3\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.15532509527209226\n",
      "Best NLL Loss : 0.0008827931966293214\n",
      "Best MSE Loss : 5.88528797752881e-05\n",
      "4\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.09902850228701948\n",
      "Best NLL Loss : 0.0003206376736354663\n",
      "Best MSE Loss : 2.137584490903109e-05\n",
      "5\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.09694546451064387\n",
      "Best NLL Loss : 0.0011393645638328265\n",
      "Best MSE Loss : 7.59576375888551e-05\n",
      "6\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1374587787859752\n",
      "Best NLL Loss : 0.0006972130694663558\n",
      "Best MSE Loss : 4.6480871297757056e-05\n",
      "7\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.0768974336106243\n",
      "Best NLL Loss : 0.0005965095892805298\n",
      "Best MSE Loss : 3.976730595203532e-05\n",
      "8\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.09361950191441404\n",
      "Best NLL Loss : 0.0003890201898862897\n",
      "Best MSE Loss : 2.5934679325752646e-05\n",
      "9\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10732685801727097\n",
      "Best NLL Loss : 0.0009367286091083158\n",
      "Best MSE Loss : 6.244857394055438e-05\n",
      "10\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1341359010546799\n",
      "Best NLL Loss : 0.0006892867144546065\n",
      "Best MSE Loss : 4.5952447630307105e-05\n",
      "11\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10398094255032553\n",
      "Best NLL Loss : 0.0005219383740434286\n",
      "Best MSE Loss : 3.4795891602895235e-05\n",
      "12\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10347683313905225\n",
      "Best NLL Loss : 0.0008352331398163005\n",
      "Best MSE Loss : 5.568220932108669e-05\n",
      "13\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.20531871930254264\n",
      "Best NLL Loss : 0.0006597185590538182\n",
      "Best MSE Loss : 4.398123727025456e-05\n",
      "14\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.13979058726644403\n",
      "Best NLL Loss : 0.0007694534338764542\n",
      "Best MSE Loss : 5.129689559176359e-05\n",
      "15\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1270120531713592\n",
      "Best NLL Loss : 0.000809246730969195\n",
      "Best MSE Loss : 5.394978206461299e-05\n",
      "16\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11200927959072068\n",
      "Best NLL Loss : 0.00044399633740279813\n",
      "Best MSE Loss : 2.9599755826853215e-05\n",
      "17\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.08185869425224172\n",
      "Best NLL Loss : 0.000702039155122046\n",
      "Best MSE Loss : 4.6802610341469724e-05\n",
      "18\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.13972860480169724\n",
      "Best NLL Loss : 0.0014174824845391235\n",
      "Best MSE Loss : 9.449883230260828e-05\n",
      "19\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10925929967774185\n",
      "Best NLL Loss : 0.0009047111118609348\n",
      "Best MSE Loss : 6.031407412406231e-05\n",
      "20\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.15695900994194642\n",
      "Best NLL Loss : 0.000763214827846413\n",
      "Best MSE Loss : 5.08809885230942e-05\n",
      "21\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1475576685208736\n",
      "Best NLL Loss : 0.001988169772996166\n",
      "Best MSE Loss : 0.00013254465153307774\n",
      "22\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.09867552616939436\n",
      "Best NLL Loss : 0.0008214136908122256\n",
      "Best MSE Loss : 5.4760912720815015e-05\n",
      "23\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11787421607376469\n",
      "Best NLL Loss : 0.0007326706459323992\n",
      "Best MSE Loss : 4.884470972882659e-05\n",
      "24\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.13187040541090003\n",
      "Best NLL Loss : 0.0011985487392770489\n",
      "Best MSE Loss : 7.99032492851366e-05\n",
      "25\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11852439084847491\n",
      "Best NLL Loss : 0.0007438903833771735\n",
      "Best MSE Loss : 4.959269222514491e-05\n",
      "26\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.150904301382101\n",
      "Best NLL Loss : 0.000823471385297299\n",
      "Best MSE Loss : 5.4898092353153284e-05\n",
      "27\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.06910730734455493\n",
      "Best NLL Loss : 0.0001713648155780343\n",
      "Best MSE Loss : 1.1424321038535622e-05\n",
      "28\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1170013655001652\n",
      "Best NLL Loss : 0.0006346706846766171\n",
      "Best MSE Loss : 4.231137897844116e-05\n",
      "29\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ELBO Loss : 0.09868114374913076\n",
      "Best NLL Loss : 0.0010275994970294787\n",
      "Best MSE Loss : 6.850663313529857e-05\n",
      "30\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.14827845443716678\n",
      "Best NLL Loss : 0.0019834125739774215\n",
      "Best MSE Loss : 0.00013222750493182815\n",
      "31\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.17812195206444653\n",
      "Best NLL Loss : 0.0008209064051833654\n",
      "Best MSE Loss : 5.472709367889105e-05\n",
      "32\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1582256576052697\n",
      "Best NLL Loss : 0.0013182547737119476\n",
      "Best MSE Loss : 8.78836515807965e-05\n",
      "33\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.14542424925589034\n",
      "Best NLL Loss : 0.0008037812343971193\n",
      "Best MSE Loss : 5.3585415626474605e-05\n",
      "34\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.07627656325023363\n",
      "Best NLL Loss : 0.0008185892083173923\n",
      "Best MSE Loss : 5.457261388782615e-05\n",
      "35\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.07304395627527652\n",
      "Best NLL Loss : 0.0002320804295744455\n",
      "Best MSE Loss : 1.5472028638296362e-05\n",
      "36\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.12567179452528196\n",
      "Best NLL Loss : 0.0010703532742582862\n",
      "Best MSE Loss : 7.135688495055245e-05\n",
      "37\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.0912639299669024\n",
      "Best NLL Loss : 0.0005137174376887004\n",
      "Best MSE Loss : 3.42478291792467e-05\n",
      "38\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.0984314411906387\n",
      "Best NLL Loss : 0.0005654620620645492\n",
      "Best MSE Loss : 3.769747080430329e-05\n",
      "39\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1257798263929298\n",
      "Best NLL Loss : 0.0017644261002830234\n",
      "Best MSE Loss : 0.0001176284066855349\n",
      "40\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11645087666012244\n",
      "Best NLL Loss : 0.001279914997879957\n",
      "Best MSE Loss : 8.53276665253305e-05\n",
      "41\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.08343447647957726\n",
      "Best NLL Loss : 0.0005187281059269604\n",
      "Best MSE Loss : 3.458187372846403e-05\n",
      "42\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.09529299497315331\n",
      "Best NLL Loss : 0.0010271922104195033\n",
      "Best MSE Loss : 6.847948069463356e-05\n",
      "43\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10328941550923477\n",
      "Best NLL Loss : 0.0011292519367072941\n",
      "Best MSE Loss : 7.52834624471529e-05\n",
      "44\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.12234901959382198\n",
      "Best NLL Loss : 0.0008859181840663449\n",
      "Best MSE Loss : 5.906121227108966e-05\n",
      "45\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.10550608497260967\n",
      "Best NLL Loss : 0.0006699970296207867\n",
      "Best MSE Loss : 4.466646864138582e-05\n",
      "46\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11718028064608685\n",
      "Best NLL Loss : 0.001566732299005835\n",
      "Best MSE Loss : 0.0001044488199337223\n",
      "47\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.1411781183773157\n",
      "Best NLL Loss : 0.0009021520874739512\n",
      "Best MSE Loss : 6.014347249826341e-05\n",
      "48\n",
      "0 30\n",
      "30 60\n",
      "60 90\n",
      "90 120\n",
      "120 150\n",
      "150 180\n",
      "180 210\n",
      "210 240\n",
      "240 270\n",
      "270 300\n",
      "300 330\n",
      "330 360\n",
      "360 390\n",
      "390 420\n",
      "420 450\n",
      "450 480\n",
      "480 510\n",
      "510 540\n",
      "540 570\n",
      "570 600\n",
      "(30, 30, 1)\n",
      "Best ELBO Loss : 0.11072904687905254\n",
      "Best NLL Loss : 0.0008175020277411956\n",
      "Best MSE Loss : 5.450013518274637e-05\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "n_var=5\n",
    "n_times=50 #no. of replicates\n",
    "time_stamp=20 #no. of timestamp\n",
    "np.random.seed(1234567) #Random seed\n",
    "#seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "average_coef_list_new=np.zeros((n_times,time_stamp))\n",
    "FDR_total_new=[]\n",
    "TPR_total_new=[]\n",
    "SHD_total_new=[]\n",
    "time_list_new=[]\n",
    "for replicate in range(n_times):\n",
    "  seed=seed_list[replicate]\n",
    "  X_all=data_create(seed,time_stamp) #create data\n",
    "  X_left=create_X(X_all)\n",
    "  D=create_D(X_all,spline_list)\n",
    "  data_all=np.append(X_left,D, axis=1)\n",
    "  print(np.shape(data_all))\n",
    "  average_list=np.zeros((time_stamp,n_var, n_var))\n",
    "  ####estimate at each time_stamp####\n",
    "  timestart_new=time.time()\n",
    "    # ----------- Configurations:\n",
    "  # ----------- Configurations:\n",
    "  n = 30 # The number of samples of data.\n",
    "  d = 30 # The number of variables in data after basis.\n",
    "  x_dims = 1 # The number of input dimensions: default 1.\n",
    "  z_dims = d # The number of latent variable dimensions: default the same as variable size.\n",
    "  epochs = 200 # Number of epochs to train.\n",
    "  batch_size = 10 # Number of samples per batch. note: should be divisible by sample size, otherwise throw an error.\n",
    "  k_max_iter = int(1e2) # The max iteration number for searching parameters.\n",
    "  original_lr = 3e-3  # Initial learning rate.\n",
    "  encoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "  decoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "  temp = 0.5 # Temperature for Gumbel softmax.\n",
    "  factor = True # Factor graph model.\n",
    "  encoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "  decoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "  tau_B = 0. # Coefficient for L-1 norm of matrix B.\n",
    "  lambda1 = 0. # Coefficient for DAG constraint h1(B).\n",
    "  lambda2 = 0. # Coefficient for identification constraint h2(B).\n",
    "  c_B = 1 # Coefficient for absolute value h1(B).\n",
    "  d_B = 1 # Coefficient for absolute value h2(B).\n",
    "  e_B = 1 # Coefficient for absolute value h3(B)\n",
    "  h1_tol = 1e-8 # The tolerance of error of h1(B) to zero.\n",
    "  h2_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "  h3_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "  lr_decay = 200 # After how many epochs to decay LR by a factor of gamma. \n",
    "  gamma = 1.0 # LR decay factor.  \n",
    "    ######################\n",
    "  p=5\n",
    "  k=5 #no.of basis\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  feat_train = torch.FloatTensor(data_all)\n",
    "  feat_valid = torch.FloatTensor(data_all)\n",
    "  feat_test = torch.FloatTensor(data_all)\n",
    "\n",
    "  # Reconstruct itself\n",
    "  train_data = TensorDataset(feat_train, feat_train)\n",
    "  valid_data = TensorDataset(feat_valid, feat_train)\n",
    "  test_data = TensorDataset(feat_test, feat_train)\n",
    "\n",
    "  train_loader = DataLoader(train_data, batch_size = batch_size)\n",
    "  valid_loader = DataLoader(valid_data, batch_size = batch_size)\n",
    "  test_loader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "  # ----------- Load modules:\n",
    "  d1=p+p*k\n",
    "  off_diag = np.ones([d1, d1]) - np.eye(d1) # Generate off-diagonal interaction graph\n",
    "  rel_rec = np.array(encode_onehot(np.where(off_diag)[1]), dtype = np.float64)\n",
    "  rel_send = np.array(encode_onehot(np.where(off_diag)[0]), dtype = np.float64)\n",
    "  rel_rec = torch.DoubleTensor(rel_rec)\n",
    "  rel_send = torch.DoubleTensor(rel_send)\n",
    "  adj_A = np.zeros((d1, d1)) # Add adjacency matrix\n",
    "\n",
    "  encoder = MLPEncoder(d1 * x_dims, x_dims, encoder_hidden,\n",
    "                          int(z_dims), adj_A,\n",
    "                          batch_size = batch_size,\n",
    "                          do_prob = encoder_dropout, factor = factor).double()\n",
    "  decoder = MLPDecoder(d1 * x_dims,\n",
    "                          z_dims, x_dims, encoder,\n",
    "                          data_variable_size = d1,\n",
    "                          batch_size = batch_size,\n",
    "                          n_hid=decoder_hidden,\n",
    "                          do_prob=decoder_dropout).double()\n",
    "\n",
    "  # ----------- Set up optimizer:\n",
    "  optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr = original_lr)\n",
    "  scheduler = lr_scheduler.StepLR(optimizer, step_size = lr_decay,\n",
    "                                  gamma = gamma)\n",
    "\n",
    "  rel_rec = Variable(rel_rec)\n",
    "  rel_send = Variable(rel_send)\n",
    "\n",
    "  # ----------- Main:\n",
    "  best_ELBO_loss = np.inf\n",
    "  best_NLL_loss = np.inf\n",
    "  best_MSE_loss = np.inf\n",
    "  h1_B_new = 1 #torch.tensor(1.)\n",
    "  h2_B_new = 1\n",
    "  h1_B_old = np.inf\n",
    "  h2_B_old = np.inf\n",
    "  lr = original_lr\n",
    "\n",
    "  try:\n",
    "      for step_k in range(k_max_iter):\n",
    "          while c_B * d_B < 1e+20:\n",
    "              for epoch in range(epochs):\n",
    "                  old_lr = lr \n",
    "                  ELBO_loss, NLL_loss, MSE_loss, origin_B, optimizer, lr = train_new(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr,p=p,k=k)\n",
    "\n",
    "                  if ELBO_loss < best_ELBO_loss:\n",
    "                      best_ELBO_loss = ELBO_loss\n",
    "\n",
    "                  if NLL_loss < best_NLL_loss:\n",
    "                      best_NLL_loss = NLL_loss\n",
    "\n",
    "                  if MSE_loss < best_MSE_loss:\n",
    "                      best_MSE_loss = MSE_loss\n",
    "\n",
    "              if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                  break\n",
    "\n",
    "              # Update parameters\n",
    "              B_new = origin_B.data.clone()\n",
    "              h1_B = fun_h1_B(B_new[p:,p:]) #acyclity on G\n",
    "              for i in range(n_timestamp):\n",
    "                  #aa=h1_B+fun_h1_B(torch.matmul(spl_consraint(spline_list,i,p),B_new[p:,:p]))#acyclity on Gamma\n",
    "                  #print(aa)\n",
    "                  output, identity=spl_consraint(spline_list,i,p)\n",
    "                  h1_B=h1_B+fun_h1_B(torch.matmul(identity.type(torch.FloatTensor),(output.type(torch.FloatTensor)*B_new[p:,:p].type(torch.FloatTensor))))\n",
    "                  #h1_B=h1_B+fun_h1_B(torch.matmul(spl_consraint(spline_list,i,p).type(torch.FloatTensor),B_new[p:,:p].type(torch.FloatTensor)))#acyclity on Gamma\n",
    "              h2_B = fun_h2_B_new(B_new) ##handle the zero on gamma and top\n",
    "              #B_trans_new=torch.transpose(B_new, 0, 1)\n",
    "              #h1_B = fun_h1_B(B_trans_new[p:,p:]) #acyclity on G\n",
    "              #h1_B = fun_h1_B(B_new[p:,:p]) #edited acyclity on G\n",
    "              #h2_B = fun_h3_B(B_new) ##handle the zero\n",
    "\n",
    "              if h1_B_new > 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                  c_B *= 10\n",
    "                  d_B *= 10\n",
    "              elif h1_B_new > 0.25 * h1_B_old and h2_B_new < 0.25 * h2_B_old:\n",
    "                  c_B *= 10\n",
    "              elif h1_B_new < 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                  d_B *= 10\n",
    "              else:\n",
    "                  break\n",
    "\n",
    "          # Update parameters    \n",
    "          h1_B_old = h1_B_new\n",
    "          h2_B_old = h2_B_new\n",
    "          lambda1 += c_B * h1_B_new\n",
    "          lambda2 += d_B * h2_B_new\n",
    "\n",
    "          if h1_B_new <= h1_tol and h2_B_new <= h2_tol:\n",
    "              break\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "      print('KeyboardInterrupt')\n",
    "\n",
    "  predB = np.matrix(origin_B.data.clone().numpy())\n",
    "  print('Best ELBO Loss :', best_ELBO_loss)\n",
    "  print('Best NLL Loss :', best_NLL_loss)\n",
    "  print('Best MSE Loss :', best_MSE_loss)\n",
    "  #calculate_effect(predB)\n",
    "  pred_gamma=predB[p:,:(p)]\n",
    "  def matrix_gen(t):\n",
    "    output,identity=spl_consraint(spline_list,t,p)\n",
    "    return torch.matmul(identity.type(torch.FloatTensor),torch.from_numpy((np.multiply(output.numpy(),pred_gamma))).type(torch.FloatTensor)).T\n",
    "  estimated_coefficient=[matrix_gen(i)[4,0].item() for i in range(time_stamp)]\n",
    "  average_coef_list_new[replicate,:]=estimated_coefficient\n",
    "  ## FDR, TPR, SHD for  new method\n",
    "  FDR_list_piece_new=[]\n",
    "  TPR_list_piece_new=[]\n",
    "  SHD_list_piece_new=[]\n",
    "  base_DAG=np.zeros((5, 5))\n",
    "  for i in range(time_stamp):\n",
    "      base_DAG[0,4]=cos(i)\n",
    "      base_graph=nx.from_numpy_matrix(base_DAG,create_using=nx.DiGraph)\n",
    "      a=abs(matrix_gen(i).numpy())\n",
    "      a[a<0.4] = 0\n",
    "      base_estimate=nx.from_numpy_matrix(a.T,create_using=nx.DiGraph)\n",
    "      FDR,TPR,SHD=count_accuracy(base_graph,base_estimate)\n",
    "      FDR_list_piece_new.append(FDR)\n",
    "      TPR_list_piece_new.append(TPR)\n",
    "      SHD_list_piece_new.append(SHD)\n",
    "  FDR_total_new.append(mean(FDR_list_piece_new))\n",
    "  TPR_total_new.append(mean(TPR_list_piece_new))\n",
    "  SHD_total_new.append(mean(SHD_list_piece_new))\n",
    "  timeend_new=time.time()\n",
    "  time_list_new.append(timeend_new-timestart_new)\n",
    "  ###write csv\n",
    "  df_new = pd.DataFrame(columns=('FDR', 'TPR',\"SHD\",\"time\"))\n",
    "  df_new[\"FDR\"]=FDR_total_new\n",
    "  df_new[\"TPR\"]=TPR_total_new\n",
    "  df_new[\"SHD\"]=SHD_total_new\n",
    "  df_new[\"time\"]=time_list_new\n",
    "  df_new.to_csv(\"cos_new.csv\")\n",
    "  print(replicate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "Hhp2IujGuYfE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "Hhp2IujGuYfE",
    "outputId": "86304788-95fa-4031-bb5c-4d69248faefb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>SHD</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.05</td>\n",
       "      <td>381.492618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.35</td>\n",
       "      <td>354.007478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>385.113245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.10</td>\n",
       "      <td>349.456397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>350.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>349.297524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>355.798350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>359.698214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.55</td>\n",
       "      <td>412.924551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>346.573471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>344.632747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>356.955603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>351.184659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>349.714365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>350.451134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>346.930725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>347.232805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>355.734798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>382.985225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>355.114313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>353.179404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>349.283098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>351.321655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>357.136569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>349.772427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>344.807625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>347.020975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>350.303371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>375.623060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>350.919682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>451.543291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>392.928627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>332.341555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.80</td>\n",
       "      <td>360.811852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>345.047358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>355.746235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>350.962285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.80</td>\n",
       "      <td>350.387862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>352.541965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>353.383152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.30</td>\n",
       "      <td>357.159978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>346.424546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.45</td>\n",
       "      <td>354.955646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>349.153637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>357.082104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>354.388087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>346.255799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>352.371958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>354.065845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>349.082133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FDR   TPR   SHD        time\n",
       "0   0.525000  0.80  1.05  381.492618\n",
       "1   0.125000  0.85  0.35  354.007478\n",
       "2   0.433333  0.60  1.10  385.113245\n",
       "3   0.416667  0.70  1.10  349.456397\n",
       "4   0.000000  0.70  0.30  350.032473\n",
       "5   0.550000  0.75  1.25  349.297524\n",
       "6   0.000000  0.70  0.30  355.798350\n",
       "7   0.000000  0.70  0.30  359.698214\n",
       "8   0.225000  0.85  0.55  412.924551\n",
       "9   0.000000  0.70  0.30  346.573471\n",
       "10  0.000000  0.75  0.25  344.632747\n",
       "11  0.075000  0.70  0.40  356.955603\n",
       "12  0.000000  0.75  0.25  351.184659\n",
       "13  0.125000  0.75  0.45  349.714365\n",
       "14  0.000000  0.30  0.70  350.451134\n",
       "15  0.000000  0.80  0.20  346.930725\n",
       "16  0.000000  0.70  0.30  347.232805\n",
       "17  0.000000  0.30  0.70  355.734798\n",
       "18  0.000000  0.80  0.20  382.985225\n",
       "19  0.000000  0.50  0.50  355.114313\n",
       "20  0.000000  0.80  0.20  353.179404\n",
       "21  0.000000  0.55  0.45  349.283098\n",
       "22  0.000000  0.55  0.45  351.321655\n",
       "23  0.025000  0.85  0.20  357.136569\n",
       "24  0.000000  0.80  0.20  349.772427\n",
       "25  0.000000  0.65  0.35  344.807625\n",
       "26  0.000000  0.70  0.30  347.020975\n",
       "27  0.000000  0.05  0.95  350.303371\n",
       "28  0.025000  0.85  0.20  375.623060\n",
       "29  0.000000  0.80  0.20  350.919682\n",
       "30  0.000000  0.80  0.20  451.543291\n",
       "31  0.000000  0.10  0.90  392.928627\n",
       "32  0.000000  0.55  0.45  332.341555\n",
       "33  0.200000  0.40  0.80  360.811852\n",
       "34  0.000000  0.45  0.55  345.047358\n",
       "35  0.741667  0.60  1.90  355.746235\n",
       "36  0.000000  0.80  0.20  350.962285\n",
       "37  0.325000  0.65  0.80  350.387862\n",
       "38  0.200000  0.75  0.55  352.541965\n",
       "39  0.000000  0.80  0.20  353.383152\n",
       "40  0.433333  0.60  1.30  357.159978\n",
       "41  0.175000  0.75  0.60  346.424546\n",
       "42  0.175000  0.80  0.45  354.955646\n",
       "43  0.000000  0.75  0.25  349.153637\n",
       "44  0.000000  0.65  0.35  357.082104\n",
       "45  0.000000  0.45  0.55  354.388087\n",
       "46  0.000000  0.75  0.25  346.255799\n",
       "47  0.000000  0.75  0.25  352.371958\n",
       "48  0.000000  0.80  0.20  354.065845\n",
       "49  0.000000  0.75  0.25  349.082133"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(columns=('FDR', 'TPR',\"SHD\",\"time\"))\n",
    "df_new[\"FDR\"]=FDR_total_new\n",
    "df_new[\"TPR\"]=TPR_total_new\n",
    "df_new[\"SHD\"]=SHD_total_new\n",
    "df_new[\"time\"]=time_list_new\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "yntv3WSQUPzc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "yntv3WSQUPzc",
    "outputId": "e3708316-bd25-4b6a-f055-cb9402b912f5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(time_stamp),np.mean(average_coef_list,0))\n",
    "plt.plot(range(time_stamp),np.mean(average_coef_list_new,0))\n",
    "plt.plot(range(time_stamp),[cos(i) for i in range(time_stamp)])\n",
    "\n",
    "# Function add a legend  \n",
    "plt.legend([\"ANOCA\",\"Estimated coeffcient\", \"True coefficient\"])\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "JwP1owSqII_6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwP1owSqII_6",
    "outputId": "50f94619-2f21-4d01-8f8a-475db31870c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDR       0.09550\n",
       "TPR       0.66500\n",
       "SHD       0.50100\n",
       "time    357.62665\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.mean()##new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "J_bKu-2Kjzj1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_bKu-2Kjzj1",
    "outputId": "ba32eea6-3135-41ee-d493-ce65702ddffd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDR      0.178255\n",
       "TPR      0.182178\n",
       "SHD      0.364844\n",
       "time    19.318386\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "GUR54HC-dpD1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUR54HC-dpD1",
    "outputId": "1aa4f7b6-514c-4001-9ede-56f31018e055"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDR       0.177833\n",
       "TPR       0.706000\n",
       "SHD       0.769000\n",
       "time    660.472313\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean() ## ANOCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "Jr8CD5mdj8ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr8CD5mdj8ba",
    "outputId": "ad304543-adde-439d-d994-d4ec55671c7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FDR      0.147714\n",
       "TPR      0.065962\n",
       "SHD      0.417706\n",
       "time    32.956139\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.std() ## ANOCA"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 100_rep_simulation_dim_5-cos_error.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
