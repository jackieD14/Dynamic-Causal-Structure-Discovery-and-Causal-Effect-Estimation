{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfee524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "D:\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from torch.optim.adam import Adam\n",
    "from utils import *\n",
    "from statistics import mean\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5584a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lsem_dynamic(W_all,Z: nx.DiGraph,\n",
    "                 n: int,n_time:int, treatment_type: str,\n",
    "                 noise_scale: float = 0.5,\n",
    "                 baseline: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Simulate samples from LSEM.\n",
    "        \n",
    "        Args:\n",
    "        W_all,A: weigthed DAG for instaneous relation and lagged relation\n",
    "        n: number of samples in each time-stamp\n",
    "        lag: degree of AR\n",
    "        n_time: number of time stamp\n",
    "        treatment_type: the type of the exposure {Binary, Gaussian}\n",
    "        noise_scale: noise scale parameter of Gaussian distribution in the lSEM\n",
    "        baseline: the baseline for the outcome\n",
    "        \n",
    "        Returns:\n",
    "        X: [time,n, d] sample matrix\n",
    "        \"\"\"\n",
    "    #W_array = nx.to_numpy_array(W)\n",
    "    Z_array = nx.to_numpy_array(Z)\n",
    "    d = Z_array.shape[0]\n",
    "    #X_all = np.zeros([n_time+1,n, d])\n",
    "    \n",
    "    ## create the initial data\n",
    "    X = np.zeros([n, d])\n",
    "    W_0=W_all[0,:,:]\n",
    "    ordered_vertices = list(nx.topological_sort(nx.from_numpy_matrix(W_0,create_using=nx.DiGraph)))\n",
    "    assert len(ordered_vertices) == d\n",
    "    rank_A = ordered_vertices.index(0)\n",
    "    for j in ordered_vertices:\n",
    "        if ordered_vertices.index(j) > rank_A:\n",
    "            parents = list(nx.from_numpy_matrix(W_0,create_using=nx.DiGraph).predecessors(j))\n",
    "            X[:, j] = X[:, parents].dot(W_0[parents, j]) + np.random.normal(scale=noise_scale, size=n)\n",
    "        elif ordered_vertices.index(j) < rank_A:\n",
    "            X[:, j] = np.random.normal(scale=noise_scale, size=n)\n",
    "        else:\n",
    "            if treatment_type == 'Binary':\n",
    "                X[:, j] = 2 * (np.random.binomial(1, 0.5, n) - 0.5)\n",
    "            elif treatment_type == 'Gaussian':\n",
    "                X[:, j] = np.random.normal(scale=noise_scale, size=n)\n",
    "            else:\n",
    "                raise ValueError('unknown exposure type')\n",
    "    X[:, d-1] += baseline\n",
    "    X_all=X\n",
    "    ## for follow-up time-stamps, X=XW+AZ+E\n",
    "    for time in range(1,n_time+1):\n",
    "        X_temp = np.matmul(X,Z_array)\n",
    "        W_array=W_all[time,:,:] ## different index!\n",
    "        W=nx.from_numpy_matrix(W_array,create_using=nx.DiGraph)\n",
    "        for j in ordered_vertices:\n",
    "            if ordered_vertices.index(j) > rank_A:\n",
    "                parents = list(W.predecessors(j))\n",
    "                X_temp[:, j] += X_temp[:, parents].dot(W_array[parents, j]) + np.random.normal(scale=noise_scale, size=n)\n",
    "            elif ordered_vertices.index(j) < rank_A:\n",
    "                X_temp[:, j] += np.random.normal(scale=noise_scale, size=n)\n",
    "            else:\n",
    "                if treatment_type == 'Binary':\n",
    "                    X_temp[:, j] += 2 * (np.random.binomial(1, 0.5, n) - 0.5)\n",
    "                elif treatment_type == 'Gaussian':\n",
    "                    X_temp[:, j] += np.random.normal(scale=noise_scale, size=n)\n",
    "                else:\n",
    "                    raise ValueError('unknown exposure type')\n",
    "        X_all=np.append(X_all,X_temp,axis=0)\n",
    "        X=X_temp\n",
    "    return X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daac612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all=np.zeros([11,5, 5])\n",
    "import math \n",
    "def cos(x):\n",
    "    return((-15+(5-x)**2)/15)\n",
    "for i in range(11):\n",
    "    W_all[i,0,4]=cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5807053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create Z matrix\n",
    "Z=np.identity(5)\n",
    "Z[0,0]=0 # no correlation for treatment\n",
    "Z_graph=nx.from_numpy_matrix(Z,create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22c4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notears_linear(X, lambda1, loss_type, max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3):\n",
    "    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "        lambda1 (float): l1 penalty parameter\n",
    "        loss_type (str): l2, logistic, poisson\n",
    "        max_iter (int): max num of dual ascent steps\n",
    "        h_tol (float): exit if |h(w_est)| <= htol\n",
    "        rho_max (float): exit if rho >= rho_max\n",
    "        w_threshold (float): drop edge if |weight| < threshold\n",
    "\n",
    "    Returns:\n",
    "        W_est (np.ndarray): [d, d] estimated DAG\n",
    "    \"\"\"\n",
    "    def _loss(W):\n",
    "        \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "        M = X @ W\n",
    "        if loss_type == 'l2':\n",
    "            R = X - M\n",
    "            loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "            G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "        elif loss_type == 'logistic':\n",
    "            loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "        elif loss_type == 'poisson':\n",
    "            S = np.exp(M)\n",
    "            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "        else:\n",
    "            raise ValueError('unknown loss type')\n",
    "        return loss, G_loss\n",
    "\n",
    "    def _h(W):\n",
    "        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "        E = slin.expm(W * W)  # (Zheng et al. 2018)\n",
    "        h = np.trace(E) - d\n",
    "        #     # A different formulation, slightly faster at the cost of numerical stability\n",
    "        #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "        #     E = np.linalg.matrix_power(M, d - 1)\n",
    "        #     h = (E.T * M).sum() - d\n",
    "        G_h = E.T * W * 2\n",
    "        return h, G_h\n",
    "\n",
    "    def _adj(w):\n",
    "        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "        return (w[:d * d] - w[d * d:]).reshape([d, d])\n",
    "\n",
    "    def _func(w):\n",
    "        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "        W = _adj(w)\n",
    "        loss, G_loss = _loss(W)\n",
    "        h, G_h = _h(W)\n",
    "        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "        G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "        g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "        return obj, g_obj\n",
    "\n",
    "    n, d = X.shape\n",
    "    w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "    bnds = [(0, 0) if i == j else (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "    if loss_type == 'l2':\n",
    "        X = X - np.mean(X, axis=0, keepdims=True)\n",
    "    for _ in range(max_iter):\n",
    "        w_new, h_new = None, None\n",
    "        while rho < rho_max:\n",
    "            sol = sopt.minimize(_func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n",
    "            w_new = sol.x\n",
    "            h_new, _ = _h(_adj(w_new))\n",
    "            if h_new > 0.25 * h:\n",
    "                rho *= 10\n",
    "            else:\n",
    "                break\n",
    "        w_est, h = w_new, h_new\n",
    "        alpha += rho * h\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = _adj(w_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbaa69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sopt\n",
    "n_times=30#no. of replicates\n",
    "time_stamp=10 #no. of timestamp\n",
    "n_var=5\n",
    "np.random.seed(1234567) #Random seed\n",
    "seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "average_coef_list_new=np.zeros((n_times,time_stamp+1,n_var,n_var))\n",
    "\n",
    "for replicate in range(n_times):\n",
    "  seed=seed_list[replicate]\n",
    "  X_all=simulate_lsem_dynamic(W_all,Z_graph,30,10, 'Binary',noise_scale=0.1).reshape(330,5,1) #create data\n",
    "  average_list=np.zeros((time_stamp+1,n_var, n_var))\n",
    "  for time in range(time_stamp+1):\n",
    "    X=X_all.reshape(330,5)[30*time:(30*time+30)]\n",
    "    average_list[time,:,:]=notears_linear(X, lambda1=0.1, loss_type=\"l2\", max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3)\n",
    "    average_coef_list_new[replicate,:,:,:]=average_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba20a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"quadra_10_30_NOTEARS_lag\",average_coef_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8175a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all=np.zeros([11,5, 5])\n",
    "def cos(x):\n",
    "    return(math.cos(x/4*math.pi)*0.8)\n",
    "for i in range(11):\n",
    "    W_all[i,0,4]=cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe3edcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sopt\n",
    "n_times=30#no. of replicates\n",
    "time_stamp=10 #no. of timestamp\n",
    "n_var=5\n",
    "np.random.seed(1234567) #Random seed\n",
    "seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "average_coef_list_new=np.zeros((n_times,time_stamp+1,n_var,n_var))\n",
    "\n",
    "for replicate in range(n_times):\n",
    "  seed=seed_list[replicate]\n",
    "  X_all=simulate_lsem_dynamic(W_all,Z_graph,30,10, 'Binary',noise_scale=0.1).reshape(330,5,1) #create data\n",
    "  average_list=np.zeros((time_stamp+1,n_var, n_var))\n",
    "  for time in range(time_stamp+1):\n",
    "    X=X_all.reshape(330,5)[30*time:(30*time+30)]\n",
    "    average_list[time,:,:]=notears_linear(X, lambda1=0.1, loss_type=\"l2\", max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3)\n",
    "    average_coef_list_new[replicate,:,:,:]=average_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cb6a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cos_10_30_NOTEARS_lag\",average_coef_list_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
