{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14283b70",
   "metadata": {
    "id": "14283b70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "D:\\Anaconda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89455d",
   "metadata": {
    "id": "1d89455d"
   },
   "source": [
    "## create basis & function demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da3d05e",
   "metadata": {
    "id": "7da3d05e"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461fc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from torch.optim.adam import Adam\n",
    "from utils import *\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcf3e1",
   "metadata": {
    "id": "e1fcf3e1"
   },
   "source": [
    "## create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df94f92c",
   "metadata": {
    "id": "df94f92c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from torch.optim.adam import Adam\n",
    "from utils import *\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b79738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lsem_dynamic(W_all,Z: nx.DiGraph,\n",
    "                 n: int,n_time:int, treatment_type: str,\n",
    "                 noise_scale: float = 0.5,\n",
    "                 baseline: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Simulate samples from LSEM.\n",
    "        \n",
    "        Args:\n",
    "        W_all,A: weigthed DAG for instaneous relation and lagged relation\n",
    "        n: number of samples in each time-stamp\n",
    "        lag: degree of AR\n",
    "        n_time: number of time stamp\n",
    "        treatment_type: the type of the exposure {Binary, Gaussian}\n",
    "        noise_scale: noise scale parameter of Gaussian distribution in the lSEM\n",
    "        baseline: the baseline for the outcome\n",
    "        \n",
    "        Returns:\n",
    "        X: [time,n, d] sample matrix\n",
    "        \"\"\"\n",
    "    #W_array = nx.to_numpy_array(W)\n",
    "    Z_array = nx.to_numpy_array(Z)\n",
    "    d = Z_array.shape[0]\n",
    "    #X_all = np.zeros([n_time+1,n, d])\n",
    "    \n",
    "    ## create the initial data\n",
    "    X = np.zeros([n, d])\n",
    "    W_0=W_all[0,:,:]\n",
    "    ordered_vertices = list(nx.topological_sort(nx.from_numpy_matrix(W_0,create_using=nx.DiGraph)))\n",
    "    assert len(ordered_vertices) == d\n",
    "    rank_A = ordered_vertices.index(0)\n",
    "    for j in ordered_vertices:\n",
    "        if ordered_vertices.index(j) > rank_A:\n",
    "            parents = list(nx.from_numpy_matrix(W_0,create_using=nx.DiGraph).predecessors(j))\n",
    "            X[:, j] = X[:, parents].dot(W_0[parents, j]) + np.random.normal(scale=noise_scale, size=n)\n",
    "        elif ordered_vertices.index(j) < rank_A:\n",
    "            X[:, j] = np.random.normal(scale=noise_scale, size=n)\n",
    "        else:\n",
    "            if treatment_type == 'Binary':\n",
    "                X[:, j] = 2 * (np.random.binomial(1, 0.5, n) - 0.5)\n",
    "            elif treatment_type == 'Gaussian':\n",
    "                X[:, j] = np.random.normal(scale=noise_scale, size=n)\n",
    "            else:\n",
    "                raise ValueError('unknown exposure type')\n",
    "    X[:, d-1] += baseline\n",
    "    X_all=X\n",
    "    ## for follow-up time-stamps, X=XW+AZ+E\n",
    "    for time in range(1,n_time+1):\n",
    "        X_temp = np.matmul(X,Z_array)\n",
    "        W_array=W_all[time,:,:] ## different index!\n",
    "        W=nx.from_numpy_matrix(W_array,create_using=nx.DiGraph)\n",
    "        for j in ordered_vertices:\n",
    "            if ordered_vertices.index(j) > rank_A:\n",
    "                parents = list(W.predecessors(j))\n",
    "                X_temp[:, j] += X_temp[:, parents].dot(W_array[parents, j]) + np.random.normal(scale=noise_scale, size=n)\n",
    "            elif ordered_vertices.index(j) < rank_A:\n",
    "                X_temp[:, j] += np.random.normal(scale=noise_scale, size=n)\n",
    "            else:\n",
    "                if treatment_type == 'Binary':\n",
    "                    X_temp[:, j] += 2 * (np.random.binomial(1, 0.5, n) - 0.5)\n",
    "                elif treatment_type == 'Gaussian':\n",
    "                    X_temp[:, j] += np.random.normal(scale=noise_scale, size=n)\n",
    "                else:\n",
    "                    raise ValueError('unknown exposure type')\n",
    "        X_all=np.append(X_all,X_temp,axis=0)\n",
    "        X=X_temp\n",
    "    return X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0681e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all=np.zeros([11,5, 5])\n",
    "import math \n",
    "def cos(x):\n",
    "    return(math.cos(x/4*math.pi)*0.8)\n",
    "for i in range(11):\n",
    "    W_all[i,0,4]=cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1048b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create Z matrix\n",
    "Z=np.identity(5)\n",
    "Z[0,0]=0 # no correlation for treatment\n",
    "Z_graph=nx.from_numpy_matrix(Z,create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b20ee4",
   "metadata": {
    "id": "94b20ee4"
   },
   "source": [
    "## piecewise ANOCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1766d8b",
   "metadata": {
    "id": "f1766d8b"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "from utils import *\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "from numpy.random import randn\n",
    "from random import seed as rseed\n",
    "from numpy.random import seed as npseed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8bde47",
   "metadata": {
    "id": "cc8bde47"
   },
   "outputs": [],
   "source": [
    "def train(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr):\n",
    "        \n",
    "        nll_train = []\n",
    "        kl_train = []\n",
    "        mse_train = []\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update optimizer\n",
    "        optimizer, lr = update_optimizer(optimizer, old_lr, c_B, d_B)\n",
    "\n",
    "        for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "            data, relations = Variable(data).double(), Variable(relations).double()\n",
    "            relations = relations.unsqueeze(2) # Reshape data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_x, logits, origin_B, adj_A_tilt_encoder, z_gap, z_positive, myA, Wa = encoder(data, rel_rec, rel_send) \n",
    "            edges = logits # Logits is of size: [num_sims, z_dims]\n",
    "\n",
    "            dec_x, output, adj_A_tilt_decoder = decoder(data, edges, d * x_dims, rel_rec, rel_send, origin_B, adj_A_tilt_encoder, Wa)\n",
    "\n",
    "            if torch.sum(output != output):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            target = data\n",
    "            preds = output\n",
    "            variance = 0.\n",
    "            \n",
    "            # Compute constraint functions h1(B) and h2(B)\n",
    "            h1_B = fun_h1_B(origin_B)\n",
    "            h2_B = fun_h2_B(origin_B)\n",
    "\n",
    "            # Reconstruction accuracy loss:\n",
    "            loss_nll = nll_gaussian(preds, target, variance)\n",
    "            # KL loss:\n",
    "            loss_kl = kl_gaussian(logits)\n",
    "            # ELBO loss:\n",
    "            loss = loss_kl + loss_nll\n",
    "            # Loss function:\n",
    "            loss += lambda1 * h1_B + 0.5 * c_B * h1_B * h1_B + lambda2 * h2_B + 0.5 * d_B * h2_B * h2_B + 100. * torch.trace(origin_B * origin_B)\n",
    "\n",
    "            loss.backward()\n",
    "            loss = optimizer.step()\n",
    "\n",
    "            myA.data = stau(myA.data, tau_B * lr)\n",
    "\n",
    "            if torch.sum(origin_B != origin_B):\n",
    "                print('nan error\\n')\n",
    "\n",
    "            mse_train.append(F.mse_loss(preds, target).item())\n",
    "            nll_train.append(loss_nll.item())\n",
    "            kl_train.append(loss_kl.item())\n",
    "\n",
    "        return np.mean(np.mean(kl_train) + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), origin_B, optimizer, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JHrk3hsw6IB4",
   "metadata": {
    "id": "JHrk3hsw6IB4"
   },
   "source": [
    "import times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "AWxKN2Pm6J6z",
   "metadata": {
    "id": "AWxKN2Pm6J6z"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c87d9f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c87d9f5",
    "outputId": "85d2eeeb-b081-4090-e0f6-ceb791280937",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913812\n",
      "Best ELBO Loss : 0.003971338889378183\n",
      "Best NLL Loss : 2.5132068481891964e-05\n",
      "Best MSE Loss : 1.0052827392756787e-05\n",
      "10\n",
      "0\n",
      "374400\n",
      "Best ELBO Loss : 0.0035763173110810224\n",
      "Best NLL Loss : 6.781875430435806e-06\n",
      "Best MSE Loss : 2.7127501721743225e-06\n",
      "10\n",
      "1\n",
      "343669\n",
      "Best ELBO Loss : 0.0028540466012661548\n",
      "Best NLL Loss : 5.0742714055034054e-06\n",
      "Best MSE Loss : 2.029708562201362e-06\n",
      "10\n",
      "2\n",
      "289095\n",
      "Best ELBO Loss : 0.0032681533517201038\n",
      "Best NLL Loss : 6.264822572078505e-06\n",
      "Best MSE Loss : 2.505929028831402e-06\n",
      "10\n",
      "3\n",
      "235846\n",
      "Best ELBO Loss : 0.0055253846556042735\n",
      "Best NLL Loss : 0.0023797141776120503\n",
      "Best MSE Loss : 0.00095188567104482\n",
      "10\n",
      "4\n",
      "432099\n",
      "Best ELBO Loss : 0.012723668552481627\n",
      "Best NLL Loss : 0.004029351922499455\n",
      "Best MSE Loss : 0.001611740768999782\n",
      "10\n",
      "5\n",
      "448097\n",
      "Best ELBO Loss : 0.0026170937372306975\n",
      "Best NLL Loss : 1.92972990046801e-06\n",
      "Best MSE Loss : 7.718919601872039e-07\n",
      "10\n",
      "6\n",
      "384097\n",
      "Best ELBO Loss : 0.0022225076936219046\n",
      "Best NLL Loss : 7.587790176688011e-06\n",
      "Best MSE Loss : 3.035116070675204e-06\n",
      "10\n",
      "7\n",
      "134030\n",
      "Best ELBO Loss : 0.004270610934846359\n",
      "Best NLL Loss : 4.950752540747908e-06\n",
      "Best MSE Loss : 1.9803010162991634e-06\n",
      "10\n",
      "8\n",
      "454991\n",
      "Best ELBO Loss : 0.0034124152125238148\n",
      "Best NLL Loss : 4.35965211356435e-06\n",
      "Best MSE Loss : 1.7438608454257405e-06\n",
      "10\n",
      "9\n",
      "401627\n",
      "Best ELBO Loss : 0.0030258239217832343\n",
      "Best NLL Loss : 0.0001313422916237426\n",
      "Best MSE Loss : 5.2536916649497036e-05\n",
      "10\n",
      "10\n",
      "97188\n",
      "Best ELBO Loss : 0.0031259690557312717\n",
      "Best NLL Loss : 1.1505631992702465e-05\n",
      "Best MSE Loss : 4.602252797080986e-06\n",
      "10\n",
      "11\n",
      "615884\n",
      "Best ELBO Loss : 0.0031299493325829034\n",
      "Best NLL Loss : 5.503574031434042e-06\n",
      "Best MSE Loss : 2.2014296125736175e-06\n",
      "10\n",
      "12\n",
      "585262\n",
      "Best ELBO Loss : 0.004798354741382609\n",
      "Best NLL Loss : 6.090270712816534e-06\n",
      "Best MSE Loss : 2.436108285126614e-06\n",
      "10\n",
      "13\n",
      "902641\n",
      "Best ELBO Loss : 0.0035869608830086715\n",
      "Best NLL Loss : 1.0290868705865861e-05\n",
      "Best MSE Loss : 4.116347482346345e-06\n",
      "10\n",
      "14\n",
      "897795\n",
      "Best ELBO Loss : 0.002073378420980026\n",
      "Best NLL Loss : 3.263243096183294e-06\n",
      "Best MSE Loss : 1.3052972384733175e-06\n",
      "10\n",
      "15\n",
      "678150\n",
      "Best ELBO Loss : 0.001960128473124709\n",
      "Best NLL Loss : 1.7140283962530239e-06\n",
      "Best MSE Loss : 6.856113585012096e-07\n",
      "10\n",
      "16\n",
      "361884\n",
      "Best ELBO Loss : 0.006258866219582562\n",
      "Best NLL Loss : 1.5126098060561853e-05\n",
      "Best MSE Loss : 6.050439224224742e-06\n",
      "10\n",
      "17\n",
      "928159\n",
      "Best ELBO Loss : 0.0031611719306496575\n",
      "Best NLL Loss : 4.252403472307297e-06\n",
      "Best MSE Loss : 1.700961388922919e-06\n",
      "10\n",
      "18\n",
      "446640\n",
      "Best ELBO Loss : 0.003843303614721595\n",
      "Best NLL Loss : 1.3262202544289446e-06\n",
      "Best MSE Loss : 5.304881017715777e-07\n",
      "10\n",
      "19\n",
      "26580\n",
      "Best ELBO Loss : 0.002714786069677197\n",
      "Best NLL Loss : 2.4391160329603526e-06\n",
      "Best MSE Loss : 9.756464131841415e-07\n",
      "10\n",
      "20\n",
      "865407\n",
      "Best ELBO Loss : 0.0031051351006793526\n",
      "Best NLL Loss : 1.0381628818758612e-05\n",
      "Best MSE Loss : 4.152651527503444e-06\n",
      "10\n",
      "21\n",
      "789523\n",
      "Best ELBO Loss : 0.00497140912002889\n",
      "Best NLL Loss : 3.7763676107866433e-06\n",
      "Best MSE Loss : 1.5105470443146568e-06\n",
      "10\n",
      "22\n",
      "704840\n",
      "Best ELBO Loss : 0.004110378599179979\n",
      "Best NLL Loss : 5.603385989114685e-06\n",
      "Best MSE Loss : 2.2413543956458737e-06\n",
      "10\n",
      "23\n",
      "359269\n",
      "Best ELBO Loss : 0.0047434240099640205\n",
      "Best NLL Loss : 8.467099082240404e-06\n",
      "Best MSE Loss : 3.3868396328961605e-06\n",
      "10\n",
      "24\n",
      "38248\n",
      "Best ELBO Loss : 0.004009637283621453\n",
      "Best NLL Loss : 5.489862441018756e-06\n",
      "Best MSE Loss : 2.195944976407503e-06\n",
      "10\n",
      "25\n",
      "809111\n",
      "Best ELBO Loss : 0.0035050374123197637\n",
      "Best NLL Loss : 2.427534518127113e-06\n",
      "Best MSE Loss : 9.71013807250845e-07\n",
      "10\n",
      "26\n",
      "137636\n",
      "Best ELBO Loss : 0.002922388491259965\n",
      "Best NLL Loss : 3.4653344943712012e-06\n",
      "Best MSE Loss : 1.3861337977484803e-06\n",
      "10\n",
      "27\n",
      "698530\n",
      "Best ELBO Loss : 0.004376066683283128\n",
      "Best NLL Loss : 9.184722503604981e-06\n",
      "Best MSE Loss : 3.6738890014419923e-06\n",
      "10\n",
      "28\n",
      "230177\n",
      "Best ELBO Loss : 0.003979933565082559\n",
      "Best NLL Loss : 6.818119991616769e-06\n",
      "Best MSE Loss : 2.7272479966467075e-06\n",
      "10\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "n = 30 # The number of samples of data.\n",
    "d = 5 # The number of variables in data.\n",
    "x_dims = 1 # The number of input dimensions: default 1.\n",
    "z_dims = d # The number of latent variable dimensions: default the same as variable size.\n",
    "epochs = 200 # Number of epochs to train.\n",
    "batch_size = 10 # Number of samples per batch. note: should be divisible by sample size, otherwise throw an error.\n",
    "n_var=5\n",
    "\n",
    "n_times=30 #no. of replicates\n",
    "time_stamp=10 #no. of timestamp\n",
    "np.random.seed(1234567) #Random seed\n",
    "seed_list=np.random.randint(1, 1000000, size=n_times)\n",
    "average_coef_list=np.zeros((n_times,time_stamp+1,n_var,n_var))\n",
    "B_list=np.zeros((n_times,d, d))\n",
    "FDR_total=[]\n",
    "TPR_total=[]\n",
    "SHD_total=[]\n",
    "time_list=[]\n",
    "for replicate in range(n_times):\n",
    "  seed=seed_list[replicate]\n",
    "  print(seed)\n",
    "  X_all=simulate_lsem_dynamic(W_all,Z_graph,30,10, 'Binary',noise_scale=0.1).reshape(330,5,1) #create data\n",
    "  average_list=np.zeros((time_stamp+1,d, d))\n",
    "  base_DAG=np.zeros((5, 5))\n",
    "  ####estimate at each time_stamp####\n",
    "  timestart=time.time()\n",
    "  #for j in range(time_stamp): ##edited to generate the output at the last time-stamp\n",
    "  for j in range(time_stamp,time_stamp+1) :\n",
    "  # ----------- Configurations:\n",
    "      k_max_iter = int(1e2) # The max iteration number for searching parameters.\n",
    "      original_lr = 3e-3  # Initial learning rate.\n",
    "      encoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "      decoder_hidden = d^2 # Number of hidden units, adaptive to dimension of nodes (d^2).\n",
    "      temp = 0.5 # Temperature for Gumbel softmax.\n",
    "      factor = True # Factor graph model.\n",
    "      encoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "      decoder_dropout = 0.0 # Dropout rate (1 - keep probability).\n",
    "      tau_B = 0. # Coefficient for L-1 norm of matrix B.\n",
    "      lambda1 = 0. # Coefficient for DAG constraint h1(B).\n",
    "      lambda2 = 0. # Coefficient for identification constraint h2(B).\n",
    "      c_B = 1 # Coefficient for absolute value h1(B).\n",
    "      d_B = 1 # Coefficient for absolute value h2(B).\n",
    "      h1_tol = 1e-8 # The tolerance of error of h1(B) to zero.\n",
    "      h2_tol = 1e-8 # The tolerance of error of h2(B) to zero.\n",
    "      lr_decay = 200 # After how many epochs to decay LR by a factor of gamma. \n",
    "      gamma = 1.0 # LR decay factor. \n",
    "      ######################\n",
    "\n",
    "\n",
    "      X=X_all[(j*30):(j*30+30),:]\n",
    "\n",
    "\n",
    "      np.random.seed(seed)\n",
    "      random.seed(seed)\n",
    "      torch.manual_seed(seed)\n",
    "      feat_train = torch.FloatTensor(X)\n",
    "      feat_valid = torch.FloatTensor(X)\n",
    "      feat_test = torch.FloatTensor(X)\n",
    "\n",
    "      # Reconstruct itself\n",
    "      train_data = TensorDataset(feat_train, feat_train)\n",
    "      valid_data = TensorDataset(feat_valid, feat_train)\n",
    "      test_data = TensorDataset(feat_test, feat_train)\n",
    "\n",
    "      train_loader = DataLoader(train_data, batch_size = batch_size)\n",
    "      valid_loader = DataLoader(valid_data, batch_size = batch_size)\n",
    "      test_loader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "      # ----------- Load modules:\n",
    "      off_diag = np.ones([d, d]) - np.eye(d) # Generate off-diagonal interaction graph\n",
    "      rel_rec = np.array(encode_onehot(np.where(off_diag)[1]), dtype = np.float64)\n",
    "      rel_send = np.array(encode_onehot(np.where(off_diag)[0]), dtype = np.float64)\n",
    "      rel_rec = torch.DoubleTensor(rel_rec)\n",
    "      rel_send = torch.DoubleTensor(rel_send)\n",
    "      adj_A = np.zeros((d, d)) # Add adjacency matrix\n",
    "\n",
    "      encoder = MLPEncoder(d * x_dims, x_dims, encoder_hidden,\n",
    "                              int(z_dims), adj_A,\n",
    "                              batch_size = batch_size,\n",
    "                              do_prob = encoder_dropout, factor = factor).double()\n",
    "      decoder = MLPDecoder(d * x_dims,\n",
    "                              z_dims, x_dims, encoder,\n",
    "                              data_variable_size = d,\n",
    "                              batch_size = batch_size,\n",
    "                              n_hid=decoder_hidden,\n",
    "                              do_prob=decoder_dropout).double()\n",
    "\n",
    "      # ----------- Set up optimizer:\n",
    "      optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr = original_lr)\n",
    "      scheduler = lr_scheduler.StepLR(optimizer, step_size = lr_decay,\n",
    "                                      gamma = gamma)\n",
    "\n",
    "      rel_rec = Variable(rel_rec)\n",
    "      rel_send = Variable(rel_send)\n",
    "\n",
    "      # ----------- Main:\n",
    "      best_ELBO_loss = np.inf\n",
    "      best_NLL_loss = np.inf\n",
    "      best_MSE_loss = np.inf\n",
    "      h1_B_new = torch.tensor(1.)\n",
    "      h2_B_new = 1\n",
    "      h1_B_old = np.inf\n",
    "      h2_B_old = np.inf\n",
    "      lr = original_lr\n",
    "\n",
    "      try:\n",
    "          for step_k in range(k_max_iter):\n",
    "              while c_B * d_B < 1e+20:\n",
    "                  for epoch in range(epochs):\n",
    "                      old_lr = lr \n",
    "                      ELBO_loss, NLL_loss, MSE_loss, origin_B, optimizer, lr = train(epoch, lambda1, c_B, lambda2, d_B, optimizer, old_lr)\n",
    "\n",
    "                      if ELBO_loss < best_ELBO_loss:\n",
    "                          best_ELBO_loss = ELBO_loss\n",
    "\n",
    "                      if NLL_loss < best_NLL_loss:\n",
    "                          best_NLL_loss = NLL_loss\n",
    "\n",
    "                      if MSE_loss < best_MSE_loss:\n",
    "                          best_MSE_loss = MSE_loss\n",
    "\n",
    "                  if ELBO_loss > 2 * best_ELBO_loss:\n",
    "                      break\n",
    "\n",
    "                  # Update parameters\n",
    "                  B_new = origin_B.data.clone()\n",
    "                  h1_B_new = fun_h1_B(B_new)\n",
    "                  h2_B_new = fun_h2_B(B_new)\n",
    "                  if h1_B_new.item() > 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                      c_B *= 10\n",
    "                      d_B *= 10\n",
    "                  elif h1_B_new.item() > 0.25 * h1_B_old and h2_B_new < 0.25 * h2_B_old:\n",
    "                      c_B *= 10\n",
    "                  elif h1_B_new.item() < 0.25 * h1_B_old and h2_B_new > 0.25 * h2_B_old:\n",
    "                      d_B *= 10\n",
    "                  else:\n",
    "                      break\n",
    "\n",
    "              # Update parameters    \n",
    "              h1_B_old = h1_B_new.item()\n",
    "              h2_B_old = h2_B_new\n",
    "              lambda1 += c_B * h1_B_new.item()\n",
    "              lambda2 += d_B * h2_B_new\n",
    "\n",
    "              if h1_B_new.item() <= h1_tol and h2_B_new <= h2_tol:\n",
    "                  break\n",
    "\n",
    "      except KeyboardInterrupt:\n",
    "          print('KeyboardInterrupt')\n",
    "\n",
    "      predB = np.matrix(origin_B.data.clone().numpy())\n",
    "      print('Best ELBO Loss :', best_ELBO_loss)\n",
    "      print('Best NLL Loss :', best_NLL_loss)\n",
    "      print('Best MSE Loss :', best_MSE_loss)\n",
    "      #calculate_effect(predB)\n",
    "      print(j)\n",
    "      average_list[j,:,:]=predB\n",
    "\n",
    "\n",
    "\n",
    "  average_coef_list[replicate,:,:,:]=average_list #average coef save to matrix\n",
    "  np.save(\"cos_10_30_ANOCA_lag_cont\",average_coef_list)\n",
    "  #df.to_csv(\"cos_rep10.csv\")\n",
    "  print(replicate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 100_rep_simulation_dim_5-cos_error.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
